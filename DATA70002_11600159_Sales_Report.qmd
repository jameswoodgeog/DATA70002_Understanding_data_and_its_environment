---
title: "Sales forecasting for the European drug store Rossmann"
subtitle: |
  \
  Word count: 1935
execute: 
  echo: false
  include: false
  eval: false
  warning: false
  message: false
# Set the formatting options
format:
  pdf:
    pdf-engine: xelatex
    fig-pos: "H"
    tbl-pos: "H"
# Include a Table Of Contents
    toc: true
# Include a List Of Figures
    lof: false
# Include a List Of Tables
    lot: false
# Number each headed section
    number-sections: false
# Set the main font size and font
    fontsize: 11pt
    mainfont: Calibri
# Adjust the borders to decrease or increase the useable page space
    geometry:
      - top = 15mm
      - bottom = 20mm
      - left = 20mm
      - right = 20mm
# Adjust the headers and footers
    header-includes:
# Set the packages to be used by LaTex
      - \usepackage{placeins}
      - \usepackage{fancyhdr}
      - \usepackage{lastpage}
#  Set the style and what goes in the header and footer of the main and all other pages.
      - \pagestyle{fancy}
      - \thispagestyle{fancy}
      - \fancyhead[R]{Student ID| 11600159}
      - \fancyhead[L]{DATA70002 | Understanding Data and its Environment}
      - \renewcommand{\headrulewidth}{0.02pt}
      - \fancypagestyle{plain}{\fancyhead[R]{Student ID| 11600159}\fancyhead[L]{DATA70002 | Understanding Data and its Environment}\fancyfoot[C]{Page \thepage\ of \pageref{LastPage}}}
# Set the page number to be "Page n of np"
      - \fancyfoot[C]{Page \thepage\ of \pageref{LastPage}}

# CSL and bibliography file to be used
bibliography: DATA70002_refs.bib
csl: "UoL_Harvard.csl"
---

```{=html}
<style>
figcaption {
    text-align: center;
}
</style>
```
```{r}
# Load the required libraries
library(tidyverse)
library(ggplot2)
library(dplyr)
library(corrplot)
library(car)
library(tree)
library(rpart)
library(caret)
library(MLmetrics)
library(randomForest)
library(xgboost)
library(kableExtra)
library(GGally)
```

\newpage

# Context

Predicting sales is a vital part for any business across all sectors, from manufacturing to retail. However, this is one of the most difficult tasks a business can undertake due to the complexities involved and the number of factors that can influence them, such as the store location, yearly seasonality, and the day of the week [@hasanAddressingSeasonalityTrend2024].

Rossmann, a part of the A S Watson group, is the market leader for health and beauty retail in Germany with around 100 stores. It also has over 4,500 stores across Europe, from Poland, Turkey to Spain, employing over 60,000 people [@aswatsongroupRossmann2024]. We have been asked to investigate predictive modelling techniques to help the company anticipate sales across the business. This report will describe the processes involved in using four predictive modelling techniques, from which one will be chosen to demonstrate the benefits of machine learning techniques in predicting sales.

# Methodology

## Data cleaning

There were three datasets to be cleaned and used in the models: -

1.  Store data
2.  Train data
3.  Test data

## Store data

```{r}
library(knitr)
library(reshape2)
# Load the data
df <- read.csv("Data/updated_training_data_with_store.csv")

# Assuming namesdf is your dataframe containing variable names
namesdf <- data.frame(Variable_Name = names(df), stringsAsFactors = FALSE)

# Calculate the number of variable names and divide them into two halves
num_vars <- nrow(namesdf)
half_num_vars <- num_vars %/% 2

# Create two columns of variable names
namesdf_split <- data.frame(Column_1 = namesdf$Variable_Name[1:half_num_vars],
                             Column_2 = namesdf$Variable_Name[(half_num_vars + 1):num_vars])

# Display the dataframe with kable and white background
namesdf_split |> 
  kable(col.names = c("Variable name", "Variable name"))  |> 
  kable_styling(bootstrap_options = c("striped", "hover"))
```

See @fig-storevars_tbl for variable names. Categorigcal variables were converted to numeric categories . Missing values were imputed using the mean of all other distances. Variables that contained too many missing values (30%) were removed and others were used to update a binary variable to show if promotion was running at the date of the observation.

\FloatBarrier

![Variables included in the 'Store' dataset](images/store_vnames.bmp){#fig-storevars_tbl fig-align="center" width="300"}

\FloatBarrier

## Train data

See @fig-train_vnames for the training dataset. The 'date' was split into three that consisted of day, month and year because each was anticipated to have a level of impact on the sales individually. If a store was not open, they were dropped. A boolean was created to show either (state) holiday or not-(state)holiday.

```{r}
traindf <- read.csv("Data/clean_train_data.csv")

# Assuming namesdf is your dataframe containing variable names
namesdf <- data.frame(Variable_Name = names(traindf), stringsAsFactors = FALSE)

# Calculate the number of variable names and divide them into two halves
num_vars <- nrow(namesdf)
half_num_vars <- num_vars %/% 2

# Create two columns of variable names
namesdf_split <- data.frame(Column_1 = namesdf$Variable_Name[1:half_num_vars],
                             Column_2 = namesdf$Variable_Name[(half_num_vars + 1):num_vars])

# Display the dataframe with kable and white background
namesdf_split |> 
  kable(col.names = c("Variable name", "Variable name"))  |> 
  kable_styling(bootstrap_options = c("striped", "hover"))
```

\FloatBarrier

![Variables included in the 'train' dataset](images/train_vnames.bmp){#fig-train_vnames fig-align="center" width="300" height="230"}

\FloatBarrier

## Test data

See @fig-test_vnames for the test data. The 'date' variable was again split into the three components. The 'open' variable contained some missing data which it was decided to infer 'open == True' for these, due to the related 'promo == T' variable.

```{r}
testdf <- read.csv("Data/clean_test_data.csv")

namesdf <- data.frame(Variable_Name = names(traindf), stringsAsFactors = FALSE)

# Calculate the number of variable names and divide them into two halves
num_vars <- nrow(namesdf)
half_num_vars <- num_vars %/% 2

# Create two columns of variable names
namesdf_split <- data.frame(Column_1 = namesdf$Variable_Name[1:half_num_vars],
                             Column_2 = namesdf$Variable_Name[(half_num_vars + 1):num_vars])

# Display the dataframe with kable and white background
namesdf_split |> 
  kable(col.names = c("Variable name", "Variable name"))  |> 
  kable_styling(bootstrap_options = c("striped", "hover"))
```

\FloatBarrier

![Variables included in the 'test' dataset](images/test_vnames.bmp){#fig-test_vnames fig-align="center" width="300"}

\FloatBarrier

```{r}
# Recode the storetype and assortment columns
table(df$StoreType)
df$StoreType <- dplyr::recode(df$StoreType, "a" = 1, "b" = 2, "c" = 3, "d" = 4)
table(df$StoreType)

table(df$Assortment)
df$Assortment <- dplyr::recode(df$Assortment, "a" = 1, "b" = 2, "c" = 3)
table(df$Assortment)
```

The 'store' and 'train' datasets were finally joined using the 'store_id' field.

# Exploratory data analysis

Distribution plots were then produced. Firstly, sales figures were plotted (@fig-sales_distributions). €0 sales values were often on a Sunday when a store was closed, so were not removed as they were related to the 'dayofweek' variable.

\FloatBarrier

```{r}
# Create a histogram of the sales figures
library(ggplot2)
ggplot(df, aes(x = Sales)) +
  geom_histogram(binwidth = 750, fill = "slateblue", color = "goldenrod", alpha = 1) +
  labs(x = "Sales value (€)",
       y = "Count") +
  theme_minimal()
```

![Distribution of historic sales](images/sales_distributions.png){#fig-sales_distributions fig-align="center" width="300"}

\FloatBarrier

@fig-cust_dist shows the distribution of customers. There were a large number of 0 values due to a store not being open on a Sunday, and the number of stores that did not open on a Sunday were large.

```{r}
# Create a histogram of the residuals using ggplot2
library(ggplot2)
ggplot(df,
       aes(x = Customers)) +
  geom_histogram(binwidth = 100,
                 fill = "slateblue",
                 color = "goldenrod",
                 alpha = 1) +
  labs(x = "Customer numbers",
       y = "Observed count") +
  theme_minimal()
```

![Distribution of historic customer counts](images/customer_distributions.png){#fig-cust_dist fig-align="center" width="300"}

\FloatBarrier

@fig-com_dists shows that the majority of stores were located relatively close to competition. Considering the nature of the business this shows that the stores are located in areas with a large number of shops around them but very few are in isolated locations by comparison to competitors.

```{r}
# Create a histogram of the residuals using ggplot2
library(ggplot2)
ggplot(df,
       aes(x = CompetitionDistance/1000)) +
  geom_histogram(binwidth = 1,
                 fill = "slateblue",
                 color = "goldenrod",
                 alpha = 1) +
  labs(x = "Competition distance (km)",
       y = "Observed count") +
  theme_minimal()
```

![Competition distances counts](images/comp_dist.png){#fig-com_dists fig-align="center" width="300"}

\FloatBarrier

A correlation plot was then produced. This can be seen in @fig-corplot_m.

```{r}
# correlation plot to find multicollinearity
numeric_df <- df[, sapply(df, is.numeric)]
cor_matrix <- cor(numeric_df)

# Save the plot as a high-resolution PNG
png("images/corplot.png",
    units = "px",
    width = 1000,
    height = 1000
    # res = 900
    )
corrplot(cor_matrix,
         type = "lower",
         method = "square",
         tl.srt = 45,
         order = "hclust",
         tl.cex = 1.5,
         tl.col = "midnightblue",
         col = COL2('BrBG', 10),
         addCoef.col = "grey50",
         number.cex = 1.45,
         cl.pos = "b",
         cl.length = 11,
         cl.ratio = 0.1)
dev.off()
```

![Correlation plot show collinearity](images/corplot.png){#fig-corplot_m fig-align="center" width="244"}

There was a relatively high correlation between the Customers/Sales variables, and the Promo variables. Those above 0.75 would be removed due to multicollinearity.

\FloatBarrier

```{r}
remove(numeric_df, cor_matrix)
```

```{r}
names(df)
```

```{r}
# create a new dataframe from df that does not include customers
mdata <- df  |> select(Store, Sales, DayOfWeek, Open, Promo, StateHoliday_Bool, Year, Month, StoreType, Assortment, CompetitionDistance, Promo2, WeekOfYear) |> 
  dplyr::arrange(Year, Month)
```

\FloatBarrier

Another correlation plot was produced just to check the validity of our removal decisions. @fig-mdata_corrplot shows that all (except WeekOfYear/Month) correlations are now under 0.75.

```{r}
# correlation plot to find multicollinearity
numeric_mdata <- mdata[, sapply(mdata, is.numeric)]
mdata_cor_matrix <- cor(numeric_mdata)

# Save the plot as a high-resolution PNG
png("images/mdata_corplot.png",
    units = "px",
    width = 1000,
    height = 1000
    # res = 900
    )
corrplot(mdata_cor_matrix,
         type = "lower",
         method = "square",
         tl.srt = 45,
         order = "hclust",
         tl.cex = 1.5,
         tl.col = "midnightblue",
         col = COL2('BrBG', 10),
         addCoef.col = "grey50",
         number.cex = 1.45,
         cl.pos = "b",
         cl.length = 11,
         cl.ratio = 0.1)
dev.off()
```

![Correlation plot after multicollinearity was addressed](images/mdata_corplot.png){#fig-mdata_corrplot fig-align="center" width="245"}

\FloatBarrier

@fig-vif_m shows the degree of Variance Inflation Factor (VIF) and that all the remaining variables' GVIF values were under 5.

```{r, eval=TRUE}
# create a linear model
formula <- Sales ~ DayOfWeek + Open + Promo + StateHoliday_Bool + Year + Month + StoreType + Assortment + CompetitionDistance + Promo2 + WeekOfYear + Store
mdata <- readRDS("Data/mdata.RDS")
# Run the model and show the summary
m <- lm(formula, data = mdata)
summary(m)
```

```{r}
library(car)
library(knitr)
library(webshot2)
# Create a kable table of the vif values
vif_m <- vif(m)
# values less than 5 are OK
# Create a kable table of the vif values with a caption
# Create kable table and save it as an HTML string
# kable_table <- kable(vif_m, caption = "Variance Inflation Factors for the linear model")

# Create kable table
kable_table <- kable(vif_m, caption = "Variance Inflation Factors for the linear model")

# Save the kable table as an HTML file
html_file <- tempfile(fileext = ".html")
save_kable(kable_table, file = html_file)

# Convert the HTML file to PNG image
image_file <- "vif_table.png"
# webshot(html_file, file = image_file)

# Optionally, remove the temporary HTML file
unlink(html_file)
```

![Variance Inflation Factors for the linear model](images/vif_table.png){#fig-vif_m fig-align="center" width="274"}

\FloatBarrier

A heat map shows the level of linearity in the relationships between the variables. and @fig-heatmap shows the majority have a low linearity.

```{r}
library(pheatmap)
library(viridisLite)

mdata |>
  cor() |>
    pheatmap(
      color = hcl.colors(100, "BrBG"),
      border_color = "grey",
      fontsize_row = 8,
      fontsize_col = 8,
      
      legend_breaks = c(-0.5, 0.25, 1),
      legend_labels = c("Low", "Medium", "High")
             )
```

![Heat map showing level of linear relationship](images/heatmap.png){#fig-heatmap fig-align="center" width="350"}

\FloatBarrier

# The Models

## Model 1: Linear regression

A linear regression model was used and @fig-lm_summary shows the results. Looking at the p-values shows all variables were significant. The R^2^ was around 0.56, sp the model can explain around 56% of the variability in the target variable (Sales in this case).

```{r}
library(car)
library(knitr)
library(webshot2)
# rerun the model without the variables that have a high VIF
# create a linear model
formula <- Sales ~ DayOfWeek + Open + Promo + StateHoliday_Bool + Year + Month + StoreType + Assortment + CompetitionDistance + Promo2 + Store
m <- lm(formula, data = mdata)
summary(m)
```

![Summary of linear regression model](images/lm_table2.png){#fig-lm_summary fig-align="center" width="274"}

\FloatBarrier

```{r}
library(ggplot2)
# hist(m$residuals)
# Create a histogram of the residuals using ggplot2
ggplot(data.frame(m$residuals), aes(x = m$residuals)) +
  geom_histogram(binwidth = 750, fill = "slateblue", color = "goldenrod", alpha = 1) +
  labs(x = "Residuals",
       y = "Frequency") +
  theme_minimal()
```

\FloatBarrier

@fig-resids_m shows the distribution of the residuals model representing the difference between the actual and predicted values. This distribution show that there is a fairly large variation between the actual and predicted values.

![Plot of residuals for the linear model](images/m_resids.png){#fig-resids_m fig-align="center" width="300"}

\FloatBarrier

The QQ-Plot (@fig-qq_m) shows how well the residuals produced by the model match what we would expect from normally distributed data. Up to a certain point (\~1.25) they match expectations, but after that, the quantiles are greater than expected. This suggests that there is a relatively large amount of data that is non-linear.

```{r}
plot(lm.good, which = 1)
```

```{r}
library(ggplot2)

# Assuming `m` is your linear model
x <- m$fitted.values
y <- m$residuals

# Create a data frame with the residuals and fitted values
data <- data.frame(fitted = x, residuals = y)

# Create a QQ plot using ggplot2
ggplot(data, aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line() +
  labs(x = "Sample quantiles",
       y = "Theoretical quantiles") +
  theme_minimal()
```

![Q-Q Plot of Residuals](images/qqplot_m.png){#fig-qq_m fig-align="center" width="300"}

\FloatBarrier

```{r}
remove(x, y)
```

The data was then split into train and validation sets with train 70% and validation 30% of the original data.

```{r}
# Split the data into training and test sets using test = up to and including 2014, and validate = /015 - This works out to about 70/30
# Sort the data by year and month
mdata <- mdata |> arrange(Year, Month)
str(mdata)
train_set <- subset(mdata, Year <= 2014)
str(train_set)
validation_set <- subset(mdata, Year >= 2015)
str(validation_set)
saveRDS(train_set, file = "Data/train_set.rds")
saveRDS(validation_set, file = "Data/validation_set.rds")
```

## Model 2: Decision Tree

Decision Tree models are non-linear, hierarchical models that use a series of decisions to produce various results, culminating in consequences based upon likelihood of, for example, the chance of an event outcome.

Firstly a `tune_grid` was created from all combinations of variables. This is part of the hyperparameter optimisation which allows for the optimal number of branches in the decision tree to be found. It was then trained using `rpart` which is a regression classification method.

```{r}
# Decision Tree
#library(rpart)
#library(caret)
#library(MLmetrics)

# use grid search for hyperparameter optimisation
tune_grid <- expand.grid(cp = seq(0.01, 0.1, by = 0.01))
# train the decision tree model
dt_model <- train(formula,
                  data = train_set,
                  method = "rpart",
                  trControl = trainControl(method = "cv",
                                           number = 5),
                  tuneGrid = tune_grid)
```

```{r}
library(caret)
# Plot the variable importances
caret::varImp(dt_model) |>
  plot()
```

\FloatBarrier

The most important variables according to the decision tree can be seen in @fig-dt_var_importance which shows open the day of the week, followed by 'Month' and 'State Holiday' were the most important in predicting sales.

![Importance of each variable to the model](images/var_importance.png){#fig-dt_var_importance fig-align="center" width="300"}

\FloatBarrier

Sales were then predicted using the validation dataset. @fig-dt_perf_values shows through the R^2^ that the model can explain 56% of the variability observed in sales.

```{r}
library(kableExtra)
library(webshot2)
library(htmlwidgets)

tree_predictions <- predict(dt_model, newdata = validation_set)
saveRDS(tree_predictions, "Data/tree_predictions.rds")
postResample(tree_predictions, validation_set$Sales)
```

![Performance statistics from the Decision Tree model](images/dt_perf_tbl.png){#fig-dt_perf_values fig-align="center"}

\FloatBarrier

```{r, eval=TRUE}
library(caret)
library(MLmetrics)
validation_set <- readRDS("Data/validation_set.rds")
tree_predictions <- readRDS("Data/tree_predictions.rds")
# Do RMSPE
# Remove missing values from actual and predicted values
dt_actual_values <- validation_set$Sales[!is.na(validation_set$Sales)]
dt_predicted_values <- tree_predictions[!is.na(validation_set$Sales)]

# Replace zero values in predicted_values with a small constant
dt_predicted_values[dt_predicted_values == 0] <- 1e-10

# Calculate RMSPE using MLmetrics package
dt_RMSPE_value <- RMSPE(dt_actual_values, dt_predicted_values)
# Print RMSPE
cat("RMSPE:", round(dt_RMSPE_value*100, 2),"%", "\n")
```

\FloatBarrier

The Root Mean Squared Percentage Error (RMSPE ) value of `r paste0(round(dt_RMSPE_value*100, 2), '%')` shows that this value is higher than the 0.5, or 50% general rule-of-thumb for a good RMSPE which is between 0.2-0.4. The RMSPE measures the predictive power by measuring the distance between the actual and predicted values. The lower, the better the model. @fig-dt_sales shows this model is predicting lower sales values than the actual observed sales.

```{r}
# Assuming your validation_set contains the Month column along with Sales
# Create a data frame with Month, Actual Sales, and Predicted Sales
dt_plot_data <- data.frame(Month = validation_set$Month,
                        Actual_Sales = validation_set$Sales,
                        Predicted_Sales = tree_predictions)

# Calculate average sales per month
avg_sales <- dt_plot_data  |> 
  group_by(Month) |> 
  summarise(Avg_Actual_Sales = mean(Actual_Sales, na.rm = TRUE),
            Avg_Predicted_Sales = mean(Predicted_Sales, na.rm = TRUE))

library(ggplot2)
# Plot using ggplot2
ggplot(avg_sales, aes(x = Month)) +
  geom_line(aes(y = Avg_Actual_Sales, color = "Actual Sales")) +
  geom_line(aes(y = Avg_Predicted_Sales, color = "Predicted Sales")) +
  labs(y = "Average Sales",
       color = "Sales Type") +
  scale_color_manual(values = c("Actual Sales" = "dodgerblue4", "Predicted Sales" = "firebrick")) +
  theme_minimal()
```

\FloatBarrier

![Average actual vs predicted sales per month using a Decision Tree model](images/act_pred_sales_month_dt.png){#fig-dt_sales fig-align="center" width="300"}

\FloatBarrier

## Model 3: Random Forest

This model contains a set of decision trees. One of the main drawbacks over the decision tree model is that it requires considerably more processing power, but as @aliRandomForestsDecision2012 point out, Decision Trees are very handy when using smaller datasets as the difference between the results would not be significant, whereas on larger datasets such as this, the differences become greater, meaning that Random Forests have better predictive power in these instances. Again, the model was trained and validated.

```{r}
# Random Forest model
library(randomForest)
rf_model <- randomForest(formula,
                         data = train_set,
                         ntree = 10,
                         mtry = 3)
saveRDS(rf_model, file = "Data/rf_model.rds")
print(rf_model)
# seems to best so far with 76.39%
```

\FloatBarrier

```{r, eval=TRUE}
library(randomForest)
rf_model <- readRDS("Data/rf_model.rds")
# Predictions
rf_predictions <- predict(rf_model, newdata = validation_set)

# evaluate the perf
postResample(rf_predictions, validation_set$Sales)

# Do RMSPE
# Remove missing values from actual and predicted values
rf_actual_values <- validation_set$Sales[!is.na(validation_set$Sales)]
rf_predicted_values <- rf_predictions[!is.na(rf_predictions)]

# Replace zero values in predicted_values with a small constant
rf_predicted_values[rf_predicted_values == 0] <- 1e-10

# Calculate RMSPE using MLmetrics package
rf_RMSPE_value <- RMSPE(rf_actual_values, rf_predicted_values)

# Print RMSPE
cat("RMSPE:", round(rf_RMSPE_value*100, 2),"%", "\n")
```

@fig-rf_perf_values shows a marked improvement in the R^2^ value, with 0.79. Effectiveness of the model shows an `r paste0('RMSPE value of ', round(rf_RMSPE_value*100, 2), '%')`. This proved to be a slight improvement from the Decision Tree model as it fell just below the 0.5, or 50% threshold.

![Performance statistics from the Random Forest model](images/rf_perf_tbl.png){#fig-rf_perf_values fig-align="center"}

\FloatBarrier

```{r}
# Assuming your validation_set contains the Month column along with Sales
# Create a data frame with Month, Actual Sales, and Predicted Sales
plot_data_rf <- data.frame(Month = validation_set$Month,
                           Actual_Sales = validation_set$Sales,
                           Predicted_Sales = rf_predictions)

# Calculate average sales per month
avg_sales_rf <- plot_data_rf |> 
  group_by(Month)  |> 
  summarise(Avg_Actual_Sales = mean(Actual_Sales, na.rm = TRUE),
            Avg_Predicted_Sales = mean(Predicted_Sales, na.rm = TRUE))

library(ggplot2)
# Plot using ggplot2
ggplot(avg_sales_rf, aes(x = Month)) +
  geom_line(aes(y = Avg_Actual_Sales, color = "Actual Sales")) +
  geom_line(aes(y = Avg_Predicted_Sales, color = "Predicted Sales")) +
  labs(y = "Average Sales",
       color = "Sales Type") +
  scale_color_manual(values = c("Actual Sales" = "dodgerblue4", "Predicted Sales" = "firebrick")) +
  theme_minimal()
```

\FloatBarrier

Comparing @fig-dt_sales above and @fig-rf_sales below, shows the gap between actual and predicted sales shrinking slightly, and significantly closer together between months 1 and 2.

![Average actual vs predicted sales per month using a Random Forest model](images/act_pred_sales_month_rf.png){#fig-rf_sales fig-align="center" width="300"}

\FloatBarrier

## Model 4: XGBoost

The eXtreme Gradient Boosting (XGBoost) model is another non-linear tree based machine learning algorithm, similar to the Decision Tree and Random Forest. The major difference is that the Random Forest will calculate the results using a number of 'trees' that have been produced in parallel, whilst the XGBoost model uses a tree that is sequentially trying to improve on itself.

```{r}
# XGBoost
library(xgboost)
str(train_set)
str(validation_set)
# -Store, -WeekOfYear
 
train_matrix <- xgb.DMatrix(data = as.matrix(train_set |> select(-Sales)), label = train_set$Sales)
    
validation_matrix <- xgb.DMatrix(data = as.matrix(validation_set |> select(-Sales, )), label = validation_set$Sales)
 
# Set parameters
params <- list(
  objective = "reg:squarederror",  # For regression tasks
  eval_metric = "rmse",            # Root Mean Squared Error
  max_depth = 6,                   # Maximum depth of a tree
  eta = 0.1,                       # Learning rate
  subsample = 0.8,                 # Subsample ratio of the training instance
  colsample_bytree = 0.8           # Subsample ratio of columns when constructing each tree
)

# Train the model
set.seed(123)  # For reproducibility
xgb_model <- xgb.train(
  params = params,
  data = train_matrix,
  nrounds = 1000,               # Number of boosting rounds
  watchlist = list(train = train_matrix, validate = validation_matrix),
  early_stopping_rounds = 10,  # Stop if no improvement after 10 rounds
  verbose = 1                  # Print training log
)

# seems to best so far with 69.32%
# Predict on validation set
xgb_predictions <- predict(xgb_model, validation_matrix)

# evaluate the perf
postResample(xgb_predictions, validation_set$Sales)
saveRDS(xgb_predictions, file = "Data/xgb_predictions.rds")
```

```{r, eval=TRUE}
library(xgboost)
xgb_predictions <- readRDS("Data/xgb_predictions.rds")
# Do RMSPE
# Remove missing values from actual and predicted values
xg_actual_values <- validation_set$Sales[!is.na(validation_set$Sales)]
xg_predicted_values <- xgb_predictions[!is.na(xgb_predictions)]

# Replace zero values in predicted_values with a small constant
xg_predicted_values[xg_predicted_values == 0] <- 1e-10

# Calculate RMSPE using MLmetrics package
xg_RMSPE_value <- RMSPE(xg_actual_values, xg_predicted_values)

# Print RMSPE
cat("RMSPE:", round(xg_RMSPE_value*100, 2),"%", "\n")
```

@fig-xg_perf_values shows the resulting R^2^ value was 0.92. In addition, the RMSPE was also calculated to `r paste0(round(xg_RMSPE_value*100, 2), '%')` This is another improvement on the previous (Random Forest) model.

\FloatBarrier

![Performance statistics for the XGBoost model](images/xg_perf_tbl.png){#fig-xg_perf_values fig-align="center"}

\FloatBarrier

```{r}
# Assuming your validation_set contains the Month column along with Sales
# Create a data frame with Month, Actual Sales, and Predicted Sales
plot_data_xgb <- data.frame(Month = validation_set$Month,
                            Actual_Sales = validation_set$Sales,
                            Predicted_Sales = xgb_predictions)

# Calculate average sales per month
avg_sales_xgb <- plot_data_xgb |> 
  group_by(Month) |> 
  summarise(Avg_Actual_Sales = mean(Actual_Sales, na.rm = TRUE),
            Avg_Predicted_Sales = mean(Predicted_Sales, na.rm = TRUE))

library(ggplot2)
# Plot using ggplot2
ggplot(avg_sales_xgb, aes(x = Month)) +
  geom_line(aes(y = Avg_Actual_Sales, color = "Actual Sales")) +
  geom_line(aes(y = Avg_Predicted_Sales, color = "Predicted Sales")) +
  labs(y = "Average Sales",
       color = "Sales Type") +
  scale_color_manual(values = c("Actual Sales" = "dodgerblue4", "Predicted Sales" = "firebrick")) +
  theme_minimal()
```

@fig-sales_xgb shows that we can see the improvements of the XGBoost model. The 1^st^ three months were relatively similar, however there is a narrowing of the gap between the actual and predicted sales for the rest of the period.

\FloatBarrier

![Actual v predicted sales from the xgboost trained model](images/act_pred_sales_month_xgb.png){#fig-sales_xgb fig-align="center" width="300"}

\FloatBarrier

## Using XGBoost to predict sales

With the above in mind, we decided that the model that provided the best predictions of 'Sales' was the XGBoost model. Therefore we would use that in the final prediction for a period where sales were unknown.

```{r}
# Final Predictions using XGB_Model
# Join the store and tesdata
test_data <- read.csv("Data/clean_test_data.csv")
store_data <- read.csv("Data/clean_store_data.csv")
```

```{r}
# Merge the data
str(test_data) #41088
str(store_data) #1115

sales_prediction_data <- merge(test_data, store_data, by = "Store") |> 
  arrange(Year, Month)

str(sales_prediction_data) #41088
```

```{r}
# Use XGBoost model to predict the sales_prediction_data from the mdata
sales_prediction_data <- sales_prediction_data  |> select(Store, DayOfWeek, Open, Promo, StateHoliday_Bool, Year, Month, StoreType, Assortment, CompetitionDistance, Promo2, WeekOfYear, Sales)
str(sales_prediction_data)

sales_prediction_data$StoreType <- dplyr::recode(sales_prediction_data$StoreType, "a" = 1, "b" = 2, "c" = 3, "d" = 4)
str(sales_prediction_data)

sales_prediction_data$Assortment <- dplyr::recode(sales_prediction_data$Assortment, "a" = 1, "b" = 2, "c" = 3)
str(sales_prediction_data)


str(mdata)
mdata$StoreType <- dplyr::recode(mdata$StoreType, "a" = 1, "b" = 2, "c" = 3, "d" = 4)
str(mdata)

mdata$Assortment <- dplyr::recode(mdata$Assortment, "a" = 1, "b" = 2, "c" = 3)
str(mdata)

# Get the model matrix data
model_matrix <- xgb.DMatrix(data = as.matrix(mdata |> select(-Sales)), label = mdata$Sales)
str(model_matrix)

sales_prediction_data$Sales[is.na(sales_prediction_data$Sales)] <- 0
str(sales_prediction_data)

sales_prediction_matrix <- xgb.DMatrix(data = as.matrix(sales_prediction_data |> select(-Sales)), label = sales_prediction_data$Sales)
str(sales_prediction_matrix)

sales_prediction_data$Sales <- predict(xgb_model, sales_prediction_matrix)

str(sales_prediction_data)
```

```{r}
# Plot the historic sales data for the stores that appear in both the actual and predicted sales data
# Get the stores that appear in both the actual sales data and predicted sales data
hist_sales <- df |>
  filter(df$Store %in% sales_prediction_data$Store)

hist_sales <- hist_sales |>
  filter(hist_sales$Month %in% sales_prediction_data$Month)

hist_sales <- hist_sales |>
  filter(hist_sales$WeekOfYear %in% sales_prediction_data$WeekOfYear)

# Get the predicted average sales per week
hist_sales <- hist_sales |>
  group_by(DayOfWeek) |>
  summarise(avg_hist_sales = mean(Sales, na.rm = TRUE))

# do the same for the sales_prediction_data
pred_avg_sales_per_week <- sales_prediction_data |>
  group_by(DayOfWeek) |>
  summarise(avg_pred_sales = mean(Sales, na.rm = TRUE))

library(ggplot2)
ggplot(hist_sales, aes(x = DayOfWeek, y = avg_hist_sales)) +
  geom_line(aes(y = avg_hist_sales,
                colour = "Actual Daily Sales")) +
  geom_line(data = pred_avg_sales_per_week,
            aes(y = avg_pred_sales, colour = "Predicted Daily Sales")) +
  scale_color_manual(values = c("Actual Daily Sales" = "dodgerblue4", "Predicted Daily Sales" = "firebrick")) +
  labs(y = "Average Sales",
       color = "Sales Type") +
  theme_minimal()
```

# Results

@fig-final_xgb shows that for each day of the week (1-7 == Monday to Sunday), the model is very accurate in predicting sales. This is important because some stores will be open or closed on different days to each other and will allow Rossmann to better decide which stores are open on which days.

![Average actual vs predicted sales per week of year using the XGBoost model](images/final_act_pred_sales_month_xgb.png){#fig-final_xgb fig-align="center" width="300"}

Further evidence of the accuracy of the model can be seen in @fig-store_sales. Each individual store on the x axis displays a predicted average sale value that is relatively close to the actual sales value.

```{r}
hist_sales <- df |>
  filter(df$Store %in% sales_prediction_data$Store)

hist_sales <- hist_sales |>
  filter(hist_sales$Month %in% sales_prediction_data$Month)

hist_sales <- hist_sales |>
  filter(hist_sales$WeekOfYear %in% sales_prediction_data$WeekOfYear)

# Get the predicted average sales per week
hist_sales <- hist_sales |>
  group_by(Store) |>
  summarise(avg_hist_sales = mean(Sales, na.rm = TRUE))

# do the same for the sales_prediction_data
pred_avg_sales_per_week <- sales_prediction_data |>
  group_by(Store) |>
  summarise(avg_pred_sales = mean(Sales, na.rm = TRUE))

library(ggplot2)
ggplot(hist_sales, aes(x = Store, y = avg_hist_sales)) +
  geom_line(aes(y = avg_hist_sales,
                colour = "Actual Daily Sales")) +
  geom_line(data = pred_avg_sales_per_week,
            aes(y = avg_pred_sales, colour = "Predicted Daily Sales")) +
  scale_color_manual(values = c("Actual Daily Sales" = "dodgerblue4", "Predicted Daily Sales" = "firebrick")) +
  labs(y = "Average Sales",
       color = "Sales Type") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

![Accuracy of predicted sales per store](images/sales_per_store.png){#fig-store_sales fig-align="center"}

# Limitations

The main limitation of the XGBoost algorithm is that, it requires greater processing power (unneccessary multi-threaded optimisation) than the other models used in this report and is more time-consuming, which means it takes it longer to run to conclusion [@maXGBoostbasedMethodFlash2021]. This would be something for a business to consider, when thinking about how to optimise sales, but that decision would have to be based on the trade-off between time and cost.

One limitation of this study overall is the lack of processing power available. If more were able to be used, it would have been pertinent to the study to compare the results of another model called Prophet, which is especially good at handling time-series data such as this.

# Benefits

The cost of utilising such a system of sales prediction however would be minimal because of the nature of using something like R, which is free and 'open-source' as used in this report. One other advantage of the XGBoost model is that it is very good at handling missing data; in fact it was designed to handle missing data well [@saraswatBeginnersTutorialXGBoost]. This could provide very useful to business when considering whether or not to open a new store, considering that sales data would be missing from a new store, making this method very useful in this context.

# Implications & recommendations

As can be seen in @fig-final_xgb and @fig-store_sales, the model highlights the importance of machine learning and it's ability to be utilised by business to predict things like sales. XGBoost is an especially good example of this and we would advise that further research is carried out, but ultimately the business should adopt either XGBoost or another similar model to predict future sales.\
\
\
\
\
\

\newpage

# References

::: {#refs}
:::
