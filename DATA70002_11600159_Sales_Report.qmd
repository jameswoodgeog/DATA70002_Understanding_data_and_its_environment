---
title: "Sales forecasting for the European drug store Rossman"
execute: 
  echo: false
  include: false
  eval: false
  warning: false
# Set the formatting options
format:
  pdf:
    pdf-engine: xelatex
# Include a Table Of Contents
    toc: true
# Include a List Of Figures
    lof: false
# Include a List Of Tables
    lot: false
# Number each headed section
    number-sections: true
# Set the main font size and font
    fontsize: 11pt
    mainfont: Calibri
# Adjust the borders to decrease or increase the useable page space
    geometry:
      - top = 15mm
      - bottom = 20mm
      - left = 20mm
      - right = 20mm
# Adjust the headers and footers
    header-includes:
# Set the packages to be used by LaTex
      - \usepackage{placeins}
      - \usepackage{fancyhdr}
      - \usepackage{lastpage}
#  Set the style and what goes in the header and footer of the main and all other pages.
      - \pagestyle{fancy}
      - \thispagestyle{fancy}
      - \fancyhead[R]{Student ID| 11600159}
      - \fancyhead[L]{DATA70002 | Understanding Data and its Environment}
      - \renewcommand{\headrulewidth}{0.02pt}
      - \fancypagestyle{plain}{\fancyhead[R]{Student ID| 11600159}\fancyhead[L]{DATA70002 | Understanding Data and its Environment}\fancyfoot[C]{Page \thepage\ of \pageref{LastPage}}}
# Set the page number to be "Page n of np"
      - \fancyfoot[C]{Page \thepage\ of \pageref{LastPage}}

# CSL and bibliography file to be used
bibliography: DATA70002_refs.bib
csl: "UoL_Harvard.csl"
---

```{=html}
<style>
figcaption {
    text-align: center;
}
</style>
```
```{r}
# Load the required libraries
library(tidyverse)
library(ggplot2)
library(dplyr)
library(corrplot)
library(car)
library(tree)
library(rpart)
library(caret)
library(MLmetrics)
library(randomForest)
library(xgboost)
library(kableExtra)
```

```{r}
# Load the data
df <- read.csv("Data/updated_training_data_with_store.csv")
str(df)
```

\newpage

# Introduction

Predicting sales is a vital part for any business across all sectors, from manufacturing, retail, logistics, to wholesale. However, this is one of the most difficult tasks a business can undertake due to the complexities involved. Sales are driven by a great deal of different factors such as the store location, proximity to competition, macro scales of yearly seasonality, to the micro scales of the time of day and the day of the week, whether there is a promotion or what the weather is doing [@hasanAddressingSeasonalityTrend2024]. All of these things influence sales in different ways, so as you can see, this makes forecasting sales the ultimate challenge for a business.

## Historic trend in sales

As a company Rossmann, a part of the A S Watson group, is the market leader for health and beauty retail in Germany with around 100 stores. It also has over 4,500 stores across Europe, from Poland, Turkey to Spain, employing over 60,000 people [@aswatsongroupRossmann2024]. We have been asked to

# Methodology

### Store data cleaning

Have a look at the structures of the stores data

Describe the steps used to

```{r}
# Recode the storetype and assortment columns
table(df$StoreType)
df$StoreType <- dplyr::recode(df$StoreType, "a" = 1, "b" = 2, "c" = 3, "d" = 4)
table(df$StoreType)

table(df$Assortment)
df$Assortment <- dplyr::recode(df$Assortment, "a" = 1, "b" = 2, "c" = 3)
table(df$Assortment)
```

```{r}
# correlation plot to find multicollinearity
numeric_df <- df[, sapply(df, is.numeric)]
cor_matrix <- cor(numeric_df)


corrplot(cor_matrix,
         type = "lower",
         method = "square",
         tl.srt = 45,
         order = "hclust",
         tl.cex = .5,
         tl.col = "midnightblue",
         col = COL2('BrBG', 10),
         addCoef.col = "grey50",
         number.cex = .45,
         cl.pos = "b",
         cl.length = 11,
         cl.ratio = 0.1)
```

![Correlation plot show collinearity](images/corplot.png)

```{r}
remove(numeric_df, cor_matrix)
```

```{r}
names(df)
```

```{r}
# create a new dataframe from df that does not include customers
mdata <- df  |> select(Store, Sales, DayOfWeek, Open, Promo, StateHoliday_Bool, Year, Month, StoreType, Assortment, CompetitionDistance, Promo2, WeekOfYear) |> 
  dplyr::arrange(Year, Month)
```

```{r fig-sales_distributions, fig.cap="Distribution of sales"}
library(ggplot2)
# hist(m$residuals)
# Create a histogram of the residuals using ggplot2
ggplot(mdata, aes(x = Sales)) +
  geom_histogram(binwidth = 750, fill = "slateblue", color = "goldenrod", alpha = 1) +
  labs(x = "Sales value (â‚¬)",
       y = "Count") +
  theme_minimal()
```

```{r, eval=TRUE}
# create a linear model
formula <- Sales ~ DayOfWeek + Open + Promo + StateHoliday_Bool + Year + Month + StoreType + Assortment + CompetitionDistance + Promo2 + WeekOfYear + Store
mdata <- readRDS("Data/mdata.RDS")
# Run the model and show the summary
m <- lm(formula, data = mdata)
summary(m)
```

```{r tbl-vif_m, include=TRUE, eval=TRUE}
library(car)
library(knitr)
# Create a kable table of the vif values
vif_m <- vif(m)
# values less than 5 are OK
# Create a kable table of the vif values with a caption
kable(vif_m, caption = "Variance Inflation Factors for the linear model")
```

```{r}
# rerun the model without the variables that have a high VIF
# create a linear model
formula <- Sales ~ DayOfWeek + Open + Promo + StateHoliday_Bool + Year + Month + StoreType + Assortment + CompetitionDistance + Promo2 + Store
m <- lm(formula, data = mdata)
summary(m)
```

```{r}
# Create a kable table of the vif values
# tells me about multicollinearity - 
# vif(m)
```

```{r}
library(ggplot2)
# hist(m$residuals)
# Create a histogram of the residuals using ggplot2
ggplot(data.frame(m$residuals), aes(x = m$residuals)) +
  geom_histogram(binwidth = 750, fill = "slateblue", color = "goldenrod", alpha = 1) +
  labs(x = "Residuals",
       y = "Frequency") +
  theme_minimal()
```

![Plot of residuals for the linear model](images/m_resids.png){#fig-resids_m}

```{r}
library(ggplot2)

# Assuming `m` is your linear model
x <- m$fitted.values
y <- m$residuals

# Create a data frame with the residuals and fitted values
data <- data.frame(fitted = x, residuals = y)

# Create a QQ plot using ggplot2
ggplot(data, aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line() +
  labs(x = "Theoretical Quantiles",
       y = "Sample Quantiles") +
  theme_minimal()
```

![QQ Plot of Residuals](images/qqplot_m.png){#fig-qq_m fig-align="center"}

```{r}
remove(x, y)
```

```{r}
# Split the data into training and test sets using test = up to and including 2014, and validate = /015 - This works out to about 70/30
# Sort the data by year and month
mdata <- mdata |> arrange(Year, Month)
str(mdata)
train_set <- subset(mdata, Year <= 2014)
str(train_set)
validation_set <- subset(mdata, Year >= 2015)
str(validation_set)
saveRDS(train_set, file = "Data/train_set.rds")
saveRDS(validation_set, file = "Data/validation_set.rds")
```

### Decision Tree

```{r}
# Decision Tree
#library(rpart)
#library(caret)
#library(MLmetrics)

# use grid search for hyperparameter optimisation
tune_grid <- expand.grid(cp = seq(0.01, 0.1, by = 0.01))
dt_model <- train(formula,
                  data = train_set,
                  method = "rpart",
                  trControl = trainControl(method = "cv",
                                           number = 5),
                  tuneGrid = tune_grid)
```

```{r}
library(caret)
# Plot the variable importances
caret::varImp(dt_model) |>
  plot()
```

![Importance of each variable to the model](images/var_importance.png)

```{r}
tree_predictions <- predict(dt_model, newdata = validation_set)
saveRDS(tree_predictions, "Data/tree_predictions.rds")
# evaluate the perf
postResample(tree_predictions, validation_set$Sales)
```

```{r, include=TRUE, eval=TRUE}
library(caret)
library(MLmetrics)
validation_set <- readRDS("Data/validation_set.rds")
tree_predictions <- readRDS("Data/tree_predictions.rds")
# Do RMSPE
# Remove missing values from actual and predicted values
dt_actual_values <- validation_set$Sales[!is.na(validation_set$Sales)]
dt_predicted_values <- tree_predictions[!is.na(validation_set$Sales)]

# Replace zero values in predicted_values with a small constant
dt_predicted_values[dt_predicted_values == 0] <- 1e-10

# Calculate RMSPE using MLmetrics package
dt_RMSPE_value <- RMSPE(dt_actual_values, dt_predicted_values)
# Print RMSPE
cat("RMSPE:", round(dt_RMSPE_value*100, 2),"%", "\n")
```

This is not such a good RMSPE as it is greater than 0.5, or 50%. General rule-of-thumb is that a good RMSPE is between 0.2-0.4.

# START HERE

```{r}
# Assuming your validation_set contains the Month column along with Sales
# Create a data frame with Month, Actual Sales, and Predicted Sales
dt_plot_data <- data.frame(Month = validation_set$Month,
                        Actual_Sales = validation_set$Sales,
                        Predicted_Sales = tree_predictions)

# Calculate average sales per month
avg_sales <- dt_plot_data  |> 
  group_by(Month) |> 
  summarise(Avg_Actual_Sales = mean(Actual_Sales, na.rm = TRUE),
            Avg_Predicted_Sales = mean(Predicted_Sales, na.rm = TRUE))

library(ggplot2)
# Plot using ggplot2
ggplot(avg_sales, aes(x = Month)) +
  geom_line(aes(y = Avg_Actual_Sales, color = "Actual Sales")) +
  geom_line(aes(y = Avg_Predicted_Sales, color = "Predicted Sales")) +
  labs(title = "Average Actual vs Predicted Sales per Month",
       y = "Average Sales",
       color = "Sales Type") +
  scale_color_manual(values = c("Actual Sales" = "dodgerblue4", "Predicted Sales" = "firebrick")) +
  theme_minimal()
```

![Average Actual vs Predicted Sales per Month](images/act_pred_sales_month.png){fig-align="center"}

### Random Forest

```{r}
# Random Forest model
# library(randomForest)
rf_model <- randomForest(formula,
                         data = train_set,
                         ntree = 10,
                         mtry = 3)

print(rf_model)
# seems to best so far with 76.39%
```

```{r}
# Predictions
rf_predictions <- predict(rf_model, newdata = validation_set)

# evaluate the perf
postResample(rf_predictions, validation_set$Sales)

# Do RMSPE
# Remove missing values from actual and predicted values
rf_actual_values <- validation_set$Sales[!is.na(validation_set$Sales)]
rf_predicted_values <- rf_predictions[!is.na(rf_predictions)]

# Replace zero values in predicted_values with a small constant
rf_predicted_values[rf_predicted_values == 0] <- 1e-10

# Calculate RMSPE using MLmetrics package
rf_RMSPE_value <- RMSPE(rf_actual_values, rf_predicted_values)

# Print RMSPE
cat("RMSPE:", round(rf_RMSPE_value*100, 2),"%", "\n")
```

```{r}
# Assuming your validation_set contains the Month column along with Sales
# Create a data frame with Month, Actual Sales, and Predicted Sales
plot_data_rf <- data.frame(Month = validation_set$Month,
                           Actual_Sales = validation_set$Sales,
                           Predicted_Sales = rf_predictions)

# Calculate average sales per month
avg_sales_rf <- plot_data_rf %>%
  group_by(Month) %>%
  summarise(Avg_Actual_Sales = mean(Actual_Sales, na.rm = TRUE),
            Avg_Predicted_Sales = mean(Predicted_Sales, na.rm = TRUE))

library(ggplot2)
# Plot using ggplot2
ggplot(avg_sales_rf, aes(x = Month)) +
  geom_line(aes(y = Avg_Actual_Sales, color = "Actual Sales")) +
  geom_line(aes(y = Avg_Predicted_Sales, color = "Predicted Sales")) +
  labs(title = "Average Actual vs Predicted Sales per Month (Random Forest)",
       y = "Average Sales",
       color = "Sales Type") +
  scale_color_manual(values = c("Actual Sales" = "dodgerblue4", "Predicted Sales" = "firebrick")) +
  theme_minimal()

```

### XGBoost

```{r}
# XGBoost
# library(xgboost)

str(train_set)
str(validation_set)
# -Store, -WeekOfYear
 
train_matrix <- xgb.DMatrix(data = as.matrix(train_set |> select(-Sales)), label = train_set$Sales)
    
validation_matrix <- xgb.DMatrix(data = as.matrix(validation_set |> select(-Sales, )), label = validation_set$Sales)
 
# Set parameters
params <- list(
  objective = "reg:squarederror",  # For regression tasks
  eval_metric = "rmse",            # Root Mean Squared Error
  max_depth = 6,                   # Maximum depth of a tree
  eta = 0.1,                       # Learning rate
  subsample = 0.8,                 # Subsample ratio of the training instance
  colsample_bytree = 0.8           # Subsample ratio of columns when constructing each tree
)
 
# # simple model which somehow lets the below model run
# set.seed(123)
# xgb_model <- xgb.train(
#   params = params,
#   data = train_matrix,
#   nrounds = 1000,
#   verbose = 1
# )
 
# Train the model
set.seed(123)  # For reproducibility
xgb_model <- xgb.train(
  params = params,
  data = train_matrix,
  nrounds = 1000,               # Number of boosting rounds
  watchlist = list(train = train_matrix, validate = validation_matrix),
  early_stopping_rounds = 10,  # Stop if no improvement after 10 rounds
  verbose = 1                  # Print training log
)

# seems to best so far with 69.32%
# Predict on validation set
xgb_predictions <- predict(xgb_model, validation_matrix)

# evaluate the perf
postResample(xgb_predictions, validation_set$Sales)
```

```{r}
# Do RMSPE
# Remove missing values from actual and predicted values
xg_actual_values <- validation_set$Sales[!is.na(validation_set$Sales)]
xg_predicted_values <- xgb_predictions[!is.na(xgb_predictions)]

# Replace zero values in predicted_values with a small constant
xg_predicted_values[xg_predicted_values == 0] <- 1e-10

# Calculate RMSPE using MLmetrics package

xg_RMSPE_value <- RMSPE(xg_actual_values, xg_predicted_values)

# Print RMSPE
cat("RMSPE:", round(xg_RMSPE_value*100, 2),"%", "\n")
```

```{r}
# Assuming your validation_set contains the Month column along with Sales
# Create a data frame with Month, Actual Sales, and Predicted Sales
plot_data_xgb <- data.frame(Month = validation_set$Month,
                            Actual_Sales = validation_set$Sales,
                            Predicted_Sales = xgb_predictions)

# Calculate average sales per month
avg_sales_xgb <- plot_data_xgb %>%
  group_by(Month) %>%
  summarise(Avg_Actual_Sales = mean(Actual_Sales, na.rm = TRUE),
            Avg_Predicted_Sales = mean(Predicted_Sales, na.rm = TRUE))

library(ggplot2)
# Plot using ggplot2
ggplot(avg_sales_xgb, aes(x = Month)) +
  geom_line(aes(y = Avg_Actual_Sales, color = "Actual Sales")) +
  geom_line(aes(y = Avg_Predicted_Sales, color = "Predicted Sales")) +
  labs(title = "Average Actual vs Predicted Sales per Month (XGBoost)",
       y = "Average Sales",
       color = "Sales Type") +
  scale_color_manual(values = c("Actual Sales" = "dodgerblue4", "Predicted Sales" = "firebrick")) +
  theme_minimal()
```

### USE XGBOOST MODEL TO PREDICT SALES

```{r}
# Final Predictions using XGB_Model
# Join the store and tesdata
test_data <- read.csv("Data/clean_test_data.csv")
store_data <- read.csv("Data/clean_store_data.csv")
```

```{r}
# Merge the data
str(test_data) #41088
str(store_data) #1115

sales_prediction_data <- merge(test_data, store_data, by = "Store") |> 
  arrange(Year, Month)

str(sales_prediction_data) #41088
```

```{r}
# Use XGBoost model to predict the sales_prediction_data from the mdata
sales_prediction_data <- sales_prediction_data  |> select(Store, DayOfWeek, Open, Promo, StateHoliday_Bool, Year, Month, StoreType, Assortment, CompetitionDistance, Promo2, WeekOfYear, Sales)
str(sales_prediction_data)

sales_prediction_data$StoreType <- dplyr::recode(sales_prediction_data$StoreType, "a" = 1, "b" = 2, "c" = 3, "d" = 4)
str(sales_prediction_data)

sales_prediction_data$Assortment <- dplyr::recode(sales_prediction_data$Assortment, "a" = 1, "b" = 2, "c" = 3)
str(sales_prediction_data)

# Get the model matrix data
model_matrix <- xgb.DMatrix(data = as.matrix(mdata |> select(-Sales)), label = mdata$Sales)
str(model_matrix)

sales_prediction_data$Sales[is.na(sales_prediction_data$Sales)] <- 0
str(sales_prediction_data)

sales_prediction_matrix <- xgb.DMatrix(data = as.matrix(sales_prediction_data |> select(-Sales)), label = sales_prediction_data$Sales)
str(sales_prediction_matrix)

sales_prediction_data$Sales <- predict(xgb_model, sales_prediction_matrix)
str(sales_prediction_data)
```

# Plotting the final sales results

```{r}
# Plot the historic sales data for the stores that appear in both the actual and predicted sales data
# Get the stores that appear in both the actual sales data and predicted sales data
hist_sales <- df |>
  filter(df$Store %in% sales_prediction_data$Store)

hist_sales <- hist_sales |>
  filter(hist_sales$Month %in% sales_prediction_data$Month)

hist_sales <- hist_sales |>
  filter(hist_sales$WeekOfYear %in% sales_prediction_data$WeekOfYear)

# Get the predicted average sales per week
hist_sales <- hist_sales |>
  group_by(DayOfWeek) |>
  summarise(avg_hist_sales = mean(Sales, na.rm = TRUE))

# do the same for the sales_prediction_data
pred_avg_sales_per_week <- sales_prediction_data |>
  group_by(DayOfWeek) |>
  summarise(avg_pred_sales = mean(Sales, na.rm = TRUE))

library(ggplot2)
ggplot(hist_sales, aes(x = DayOfWeek, y = avg_hist_sales)) +
  geom_line(aes(y = avg_hist_sales,
                colour = "Actual Daily Sales")) +
  geom_line(data = pred_avg_sales_per_week,
            aes(y = avg_pred_sales, colour = "Predicted Daily Sales")) +
  scale_color_manual(values = c("Actual Daily Sales" = "dodgerblue4", "Predicted Daily Sales" = "firebrick")) +
  labs(title = "Average Actual vs Predicted Sales per Week of Year",
       y = "Average Sales",
       color = "Sales Type")
```

Review the available data and describe it in terms of its variables, quality, and relevance to the sales forecasting

Link data sets together as appropriate

Pre-process the data as appropriate for further analytics, for example, you may want to encode any categorical data, create new variables, identify how many missing values there are and deal with them appropriately, etc.

Identify the key factors affecting sales, for example, you may want to check whether competition and promotions have an impact on sales, and how public holidays cause sales fluctuations.

Build a forecasting model (which can be a linear regression model, a neural network model or something else) using the variables you identified. Please make sure to justify the choice of your modelling approach.

Use the Root Mean Square Percentage Error (RMSPE) to forecast accuracy

# Results

Interpret key results, assumptions and limitations of your analysis.

# Conclusion

## Limitations

## Implications

## Recommendations

# References
