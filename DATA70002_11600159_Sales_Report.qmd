---
title: "Sales forecasting for the European drug store Rossman"
execute: 
  echo: false
  include: false
  eval: false
  warning: false
  message: false
# Set the formatting options
format:
  pdf:
    pdf-engine: xelatex
# Include a Table Of Contents
    toc: true
# Include a List Of Figures
    lof: false
# Include a List Of Tables
    lot: false
# Number each headed section
    number-sections: true
# Set the main font size and font
    fontsize: 11pt
    mainfont: Calibri
# Adjust the borders to decrease or increase the useable page space
    geometry:
      - top = 15mm
      - bottom = 20mm
      - left = 20mm
      - right = 20mm
# Adjust the headers and footers
    header-includes:
# Set the packages to be used by LaTex
      - \usepackage{placeins}
      - \usepackage{fancyhdr}
      - \usepackage{lastpage}
#  Set the style and what goes in the header and footer of the main and all other pages.
      - \pagestyle{fancy}
      - \thispagestyle{fancy}
      - \fancyhead[R]{Student ID| 11600159}
      - \fancyhead[L]{DATA70002 | Understanding Data and its Environment}
      - \renewcommand{\headrulewidth}{0.02pt}
      - \fancypagestyle{plain}{\fancyhead[R]{Student ID| 11600159}\fancyhead[L]{DATA70002 | Understanding Data and its Environment}\fancyfoot[C]{Page \thepage\ of \pageref{LastPage}}}
# Set the page number to be "Page n of np"
      - \fancyfoot[C]{Page \thepage\ of \pageref{LastPage}}

# CSL and bibliography file to be used
bibliography: DATA70002_refs.bib
csl: "UoL_Harvard.csl"
---

```{=html}
<style>
figcaption {
    text-align: center;
}
</style>
```
```{r}
# Load the required libraries
library(tidyverse)
library(ggplot2)
library(dplyr)
library(corrplot)
library(car)
library(tree)
library(rpart)
library(caret)
library(MLmetrics)
library(randomForest)
library(xgboost)
library(kableExtra)
library(GGally)
```

\newpage

# Introduction

Predicting sales is a vital part for any business across all sectors, from manufacturing, retail, logistics, to wholesale. However, this is one of the most difficult tasks a business can undertake due to the complexities involved. Sales are driven by a great deal of different factors such as the store location, proximity to competition, macro scales of yearly seasonality, to the micro scales of the time of day and the day of the week, whether there was a promotion or what the weather is doing [@hasanAddressingSeasonalityTrend2024]. All of these things influence sales in different ways, so as you can see, this makes forecasting sales the ultimate challenge for a business.

## Historic trend in sales

As a company Rossmann, a part of the A S Watson group, is the market leader for health and beauty retail in Germany with around 100 stores. It also has over 4,500 stores across Europe, from Poland, Turkey to Spain, employing over 60,000 people [@aswatsongroupRossmann2024]. We have been asked to

# Methodology

## Data cleaning

The following will describe the processes involved in the preparation required to enable the data to be used in the various modelling techniques. There were three datasets provided: -

1.  Store data
2.  Train data
3.  Test data

Each required various and different cleaning and preparation steps and each will be set out below.

### Store data

It was decided that the changes to be made to the 'store type' and 'assortment' (category of the range of products held by a store) would be converted to numeric categories from alpha-characters (a, b, c, etc). This was so that the predictive power of the model(s) were as good as possible and the data was easier to manipulate. Missing values for the 'competition distance' variable were imputed using the mean of all other distances. The 'competition open since month' and 'competition open since year' variables contained too many missing values (30%), and there was no corresponding variable in the other data, so was removed entirely. It was considered that another option would have been to impute the missing data with an estimation calculated from the other data, however, this was in the end discarded due to the inevitable inaccuracy that would have been introduced considering the number of missing values. The 'promo2sinceweek', 'promo2sinceyear' and 'PromoInterval' variables were amended so that the binary was updated in accordance with whether there was a promotion running at the date of the observation.

### Train data

The train dataset would be used to train the chosen models that would predict sales required cleaning as follows. The 'dayofweek' variable was missing a relatively small number of observations which were also randomly distributed throughout the entire dataset. This meant that the decision was made not to impute the missing values. The 'date' variable was split into 3 new variables (keeping the original) that consisted of one each for day, month and year. This was done because we envisaged that each would have a separate and differing level of impact on the sales. Within the 'open' variable, there were a number of stores that were stated as 'open == 0'. The rows for these observations were dropped from the data as the store remained closed throughout and sales would therefore skew towards 0. The 'stateholiday' variable required the creation of two new variables derived from it. The first was changing 'none' to d so that it could be treated as a categorical variable, and the second was to create one that contained a boolean for either holiday or not holiday. It was considered that the Christmas and Easter holidays could skew sales, but not sufficiently to force them to be treated differently from other state holidays.

### Test data

Once the best sales prediction model was chosen, it would have to be tested on data that we 'did not know' the sales for. This required cleaning so that it was as useful as possible. The 'date' variable was split into the three components (day, month, year). again, because we believed that each would have a different impact on sales. he 'open' variable in this data contained some missing data (4). It was decided to infer 'open == True' for these, due to the related 'promo == T' variable.

```{r}
# Load the data
df <- read.csv("Data/updated_training_data_with_store.csv")
str(df)
```

```{r}
# Recode the storetype and assortment columns
table(df$StoreType)
df$StoreType <- dplyr::recode(df$StoreType, "a" = 1, "b" = 2, "c" = 3, "d" = 4)
table(df$StoreType)

table(df$Assortment)
df$Assortment <- dplyr::recode(df$Assortment, "a" = 1, "b" = 2, "c" = 3)
table(df$Assortment)
```

After each dataset had been cleaned, and all data types had been converted etc., the 'store' and 'train' datasets needed to be joined. This was done using a simple join on the 'store_id' field.

## Exploratory data anaysis

Once the data had been cleaned, various simple plots were produced so that distributions and outliers could be considered if observed.

Firstly, a distribution of the sales figures was plotted and can be seen in @fig-sales_distributions. It shows that there are a large number of €0 sales values. These were often on a Sunday when a store was closed, so were not removed as they were related to the dayofweek variable and were not going to skew results, in fact were vital for a more accurate prediction. All other sales results were relatively normally distributed.

```{r}
# Create a histogram of the sales figures
library(ggplot2)
ggplot(df, aes(x = Sales)) +
  geom_histogram(binwidth = 750, fill = "slateblue", color = "goldenrod", alpha = 1) +
  labs(x = "Sales value (€)",
       y = "Count") +
  theme_minimal()
```

![Distribution of historic sales](images/sales_distributions.png){#fig-sales_distributions width="300"}

\FloatBarrier

@fig-cust_dist shows the distribution of customer counts. Again, there were a large number of 0 values, however these are due to a store not being open on a Sunday, and the number of stores that did not open on a Sunday were large. Most stores observed between 0 and around 2500 customers in total.

```{r}
# Create a histogram of the residuals using ggplot2
library(ggplot2)
ggplot(df,
       aes(x = Customers)) +
  geom_histogram(binwidth = 100,
                 fill = "slateblue",
                 color = "goldenrod",
                 alpha = 1) +
  labs(x = "Customer numbers",
       y = "Observed count") +
  theme_minimal()
```

![Distribution of historic customer counts](images/customer_distributions.png){#fig-cust_dist width="300"}

\FloatBarrier

@fig-com_dists shows that the majority of stores were located relatively close to competition. Considering the nature of the business this shows both that the stores are located in areas with a large number of shops around them, so for example, high streets etc., but also that very few are in isolated locations by comparison to competitors.

```{r}
# Create a histogram of the residuals using ggplot2
library(ggplot2)
ggplot(df,
       aes(x = CompetitionDistance/1000)) +
  geom_histogram(binwidth = 1,
                 fill = "slateblue",
                 color = "goldenrod",
                 alpha = 1) +
  labs(x = "Competition distance (km)",
       y = "Observed count") +
  theme_minimal()
```

![Competition distances counts](images/comp_dist.png){#fig-com_dists width="300"}

\FloatBarrier

Once these initial observations had been carried out, and we were happy that the data looked reasonable and fit for modelling, so that we could visualise the correlation between the variables, a correlation plot was produced. This can be seen in @fig-corplot_m.

```{r}
# correlation plot to find multicollinearity
numeric_df <- df[, sapply(df, is.numeric)]
cor_matrix <- cor(numeric_df)

# Save the plot as a high-resolution PNG
png("images/corplot.png",
    units = "px",
    width = 1000,
    height = 1000
    # res = 900
    )
corrplot(cor_matrix,
         type = "lower",
         method = "square",
         tl.srt = 45,
         order = "hclust",
         tl.cex = 1.5,
         tl.col = "midnightblue",
         col = COL2('BrBG', 10),
         addCoef.col = "grey50",
         number.cex = 1.45,
         cl.pos = "b",
         cl.length = 11,
         cl.ratio = 0.1)
dev.off()
```

![Correlation plot show collinearity](images/corplot.png){#fig-corplot_m width="244"}

Here we can see that there was a relatively high correlation between the Customers/Sales variables, and the Promo2/Promo2SinceYear/ YearPromo/Promo2SinceWeek variables. Variables to be removed would be decided upon whether the correlation was above 0.75. This allowed us to remove some of these variables that displayed collinearity from the model. For example, Sales could easily be predicted by Customers, however, there are many factors at play in addition to this. In order to draw meaningful conclusions from our models, the 'Customers' and other variables displaying collinearity were removed (we couldn't remove 'Sales' as this was required to be the dependent variable in our model(s)). However, the one exception to this was to leave the WeekOfYear/Month relationship in the model, as these would be important. For example they would naturally be correlated due to the 1^st^ week of the year always appearing in January, and this would affect sales as well.

\FloatBarrier

```{r}
remove(numeric_df, cor_matrix)
```

```{r}
names(df)
```

```{r}
# create a new dataframe from df that does not include customers
mdata <- df  |> select(Store, Sales, DayOfWeek, Open, Promo, StateHoliday_Bool, Year, Month, StoreType, Assortment, CompetitionDistance, Promo2, WeekOfYear) |> 
  dplyr::arrange(Year, Month)
```

\FloatBarrier

Once all of the variables had been removed, another correlation plot was produced just to check the validity of our decisions. This can be seen in @fig-mdata_corrplot. This shows that all (except WeekOfYear/Month) correlations are now under 0.75 and do not have near perfect linear relationships.

```{r}
# correlation plot to find multicollinearity
numeric_mdata <- mdata[, sapply(mdata, is.numeric)]
mdata_cor_matrix <- cor(numeric_mdata)

# Save the plot as a high-resolution PNG
png("images/mdata_corplot.png",
    units = "px",
    width = 1000,
    height = 1000
    # res = 900
    )
corrplot(mdata_cor_matrix,
         type = "lower",
         method = "square",
         tl.srt = 45,
         order = "hclust",
         tl.cex = 1.5,
         tl.col = "midnightblue",
         col = COL2('BrBG', 10),
         addCoef.col = "grey50",
         number.cex = 1.45,
         cl.pos = "b",
         cl.length = 11,
         cl.ratio = 0.1)
dev.off()
```

![Correlation plot after multicollinearity was addressed](images/mdata_corplot.png){#fig-mdata_corrplot width="245"}

\FloatBarrier

@tbl-vif_m shows the remaining variables and their degree of Variance Inflation Factor (VIF). This was showing that all the remaining variables GVIF values are under 5, and points to the measure of the relationship between each variable [@akinwandeVarianceInflationFactor2015].

```{r, eval=TRUE}
# create a linear model
formula <- Sales ~ DayOfWeek + Open + Promo + StateHoliday_Bool + Year + Month + StoreType + Assortment + CompetitionDistance + Promo2 + WeekOfYear + Store
mdata <- readRDS("Data/mdata.RDS")
# Run the model and show the summary
m <- lm(formula, data = mdata)
summary(m)
```

```{r}
library(car)
library(knitr)
library(webshot2)
# Create a kable table of the vif values
vif_m <- vif(m)
# values less than 5 are OK
# Create a kable table of the vif values with a caption
# Create kable table and save it as an HTML string
# kable_table <- kable(vif_m, caption = "Variance Inflation Factors for the linear model")

# Create kable table
kable_table <- kable(vif_m, caption = "Variance Inflation Factors for the linear model")

# Save the kable table as an HTML file
html_file <- tempfile(fileext = ".html")
save_kable(kable_table, file = html_file)

# Convert the HTML file to PNG image
image_file <- "vif_table.png"
# webshot(html_file, file = image_file)

# Optionally, remove the temporary HTML file
unlink(html_file)
```

![Variance Inflation Factors for the linear model](images/vif_table.png){#tbl-vif_m fig-align="center" width="274"}

\FloatBarrier

The final visualisation to be used in order that the correct modelling method was used, was to produce a heat map that shows the level of linearity in the relationships between the variables. As can be seen in @fig-heatmap map the majority of the relationships have a low linearity.

```{r}
library(pheatmap)
library(viridisLite)

mdata |>
  cor() |>
    pheatmap(
      color = hcl.colors(100, "BrBG"),
      border_color = "grey",
      fontsize_row = 8,
      fontsize_col = 8,
      
      legend_breaks = c(-0.5, 0.25, 1),
      legend_labels = c("Low", "Medium", "High")
             )
```

![Heat map showing level of linear relationship](images/heatmap.png){#fig-heatmap width="350"}

\FloatBarrier

Once this table was analysed and we were happy that the remaining variables would not skew the results of any regression or other predictive models, we could then attempt to investigate which of the chosen models was able to best predict sales.

The first model to be used to predict sales was a simple linear regression model. Each variable was used to predict sales and @fig-lm_summary shows the results of the model. As we can see, by looking at the p-values, it was clear that all variables included in the model are significant, or that the probability of obtaining the observed results by chance was very low. The R^2^ value was around 0.56, or that the model can explain around 56% of the variability in the target variable (Sales in this case).

```{r}
library(car)
library(knitr)
library(webshot2)
# rerun the model without the variables that have a high VIF
# create a linear model
formula <- Sales ~ DayOfWeek + Open + Promo + StateHoliday_Bool + Year + Month + StoreType + Assortment + CompetitionDistance + Promo2 + Store
m <- lm(formula, data = mdata)
summary(m)
```

![Summary of linear regression model](images/lm_table2.png){#fig-lm_summary width="274"}

\FloatBarrier

```{r}
# Create a kable table of the vif values
# tells me about multicollinearity - 
# vif(m)

```

```{r}
library(ggplot2)
# hist(m$residuals)
# Create a histogram of the residuals using ggplot2
ggplot(data.frame(m$residuals), aes(x = m$residuals)) +
  geom_histogram(binwidth = 750, fill = "slateblue", color = "goldenrod", alpha = 1) +
  labs(x = "Residuals",
       y = "Frequency") +
  theme_minimal()
```

\FloatBarrier

@fig-resids_m shows the distribution of the residuals from the linear regression model. Residuals represent the difference between the actual values and those predicted by the model. This distribution of the residuals here show that there is a fairly large variation between the actual and predicted values.

![Plot of residuals for the linear model](images/m_resids.png){#fig-resids_m width="300"}

\FloatBarrier

Another useful way to visualise the effectiveness of the model to predict sales is to produce a QQ-Plot. The plot in @fig-qq_m effectively plots how well the residuals produced by the model match what we would expect from normally distributed data. As we can see it shows that up to a certain point (\~1.25) match what we would expect from a normal distribution, and after that point the quantiles are greater than expected from normally distributed data. This suggests that there is a relatively large amount of data that is non-linear.

```{r}
plot(lm.good, which = 1)
```

```{r}
library(ggplot2)

# Assuming `m` is your linear model
x <- m$fitted.values
y <- m$residuals

# Create a data frame with the residuals and fitted values
data <- data.frame(fitted = x, residuals = y)

# Create a QQ plot using ggplot2
ggplot(data, aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line() +
  labs(x = "Sample quantiles",
       y = "Theoretical quantiles") +
  theme_minimal()
```

![Q-Q Plot of Residuals](images/qqplot_m.png){#fig-qq_m fig-align="center" width="300"}

\FloatBarrier

```{r}
remove(x, y)
```

Before we could attempt to produce results from various non-linear modelling techniques, we had to split the data into separate train and validation sets. The train data consisted of around 70% of the data and the validation data consisted of the remaining 30%.

```{r}
# Split the data into training and test sets using test = up to and including 2014, and validate = /015 - This works out to about 70/30
# Sort the data by year and month
mdata <- mdata |> arrange(Year, Month)
str(mdata)
train_set <- subset(mdata, Year <= 2014)
str(train_set)
validation_set <- subset(mdata, Year >= 2015)
str(validation_set)
saveRDS(train_set, file = "Data/train_set.rds")
saveRDS(validation_set, file = "Data/validation_set.rds")
```

### Decision Tree

The first non-linear model to be tested is know as a Decision Tree. These are effectively hierarchical models that use a series of decisions to produce various results, culminating in consequences of those decisions based upon likelihood of things like the chance of event outcomes for example.

Firstly a `tune_grid` was created to create a dataframe from all combinations of the variables in the data provided to the model. This is part of the hyperparameter optimisation which attempts to yield the optimal number of branches in the decision tree. The model was then trained using the `rpart` method which is a regression classification method, on the train dataset

```{r}
# Decision Tree
#library(rpart)
#library(caret)
#library(MLmetrics)

# use grid search for hyperparameter optimisation
tune_grid <- expand.grid(cp = seq(0.01, 0.1, by = 0.01))
# train the decision tree model
dt_model <- train(formula,
                  data = train_set,
                  method = "rpart",
                  trControl = trainControl(method = "cv",
                                           number = 5),
                  tuneGrid = tune_grid)
```

```{r}
library(caret)
# Plot the variable importances
caret::varImp(dt_model) |>
  plot()
```

\FloatBarrier

Using the model, we could observe the most important variables according to the decision tree. As you can see in @fig-dt_var_importance, it is no surprise that the most important variable to predict (any sales at all) was whether a store was open. Following that, the day of the week was the next most important predictor of sales values, followed by 'Month' and 'State Holiday' (or whether there was a state holiday on a particular day.

![Importance of each variable to the model](images/var_importance.png){#fig-dt_var_importance width="300"}

\FloatBarrier

Once the training had been completed, sales were predicted from the model using the validation dataset as previously mentioned, and the performance evaluated. As can be seen in @tbl-dt_perf_values, the R^2^ was 0.56, which is another way of saying that the model can explain 56% of the variability observed in the model of the target variable ('Sales').

```{r}
library(kableExtra)
library(webshot2)
library(htmlwidgets)

tree_predictions <- predict(dt_model, newdata = validation_set)
saveRDS(tree_predictions, "Data/tree_predictions.rds")
postResample(tree_predictions, validation_set$Sales)
```

![Performance statistics from the Decision Tree model](images/dt_perf_tbl.png){#tbl-dt_perf_values}

\FloatBarrier

```{r, eval=TRUE}
library(caret)
library(MLmetrics)
validation_set <- readRDS("Data/validation_set.rds")
tree_predictions <- readRDS("Data/tree_predictions.rds")
# Do RMSPE
# Remove missing values from actual and predicted values
dt_actual_values <- validation_set$Sales[!is.na(validation_set$Sales)]
dt_predicted_values <- tree_predictions[!is.na(validation_set$Sales)]

# Replace zero values in predicted_values with a small constant
dt_predicted_values[dt_predicted_values == 0] <- 1e-10

# Calculate RMSPE using MLmetrics package
dt_RMSPE_value <- RMSPE(dt_actual_values, dt_predicted_values)
# Print RMSPE
cat("RMSPE:", round(dt_RMSPE_value*100, 2),"%", "\n")
```

\FloatBarrier

Once the calculations were done, the final RMSPE value of `r paste0(round(dt_RMSPE_value*100, 2), '%')` was found. This value is higher than the 0.5, or 50% general rule-of-thumb for a good RMSPE which is between 0.2-0.4. The Route Means Square Percentage Error essentially measures the predictive power of a model by measuring the distance between the actual and predicted values in the data. Therefore, the lower the distance, the better the model is at predicting, in this instance; sales. As @fig-dt_sales shows, the model is overall, predicting lower sales values than the actual observed sales.

```{r}
# Assuming your validation_set contains the Month column along with Sales
# Create a data frame with Month, Actual Sales, and Predicted Sales
dt_plot_data <- data.frame(Month = validation_set$Month,
                        Actual_Sales = validation_set$Sales,
                        Predicted_Sales = tree_predictions)

# Calculate average sales per month
avg_sales <- dt_plot_data  |> 
  group_by(Month) |> 
  summarise(Avg_Actual_Sales = mean(Actual_Sales, na.rm = TRUE),
            Avg_Predicted_Sales = mean(Predicted_Sales, na.rm = TRUE))

library(ggplot2)
# Plot using ggplot2
ggplot(avg_sales, aes(x = Month)) +
  geom_line(aes(y = Avg_Actual_Sales, color = "Actual Sales")) +
  geom_line(aes(y = Avg_Predicted_Sales, color = "Predicted Sales")) +
  labs(y = "Average Sales",
       color = "Sales Type") +
  scale_color_manual(values = c("Actual Sales" = "dodgerblue4", "Predicted Sales" = "firebrick")) +
  theme_minimal()
```

\FloatBarrier

![Average actual vs predicted sales per month using a Decision Tree model](images/act_pred_sales_month_dt.png){#fig-dt_sales fig-align="center" width="300"}

\FloatBarrier

### Random Forest

We then tested the predictive power of another non-linear modelling technique called "Random Forest". This is essentially a model that contains a set of decision trees. One of the main drawbacks over the decision tree model is that it requires considerably more processing power, but as @aliRandomForestsDecision2012 point out, Decision Trees are very handy when using smaller datasets as the difference between the results would not be significant, whereas on larger datasets, the differences become greater, meaning that Random Forests have better predictive power in these instances.

Again, the model was trained on the same `train` data, and validated against the same `validation` datasets.

```{r}
# Random Forest model
library(randomForest)
rf_model <- randomForest(formula,
                         data = train_set,
                         ntree = 10,
                         mtry = 3)
saveRDS(rf_model, file = "Data/rf_model.rds")
print(rf_model)
# seems to best so far with 76.39%
```

\FloatBarrier

```{r, eval=TRUE}
library(randomForest)
rf_model <- readRDS("Data/rf_model.rds")
# Predictions
rf_predictions <- predict(rf_model, newdata = validation_set)

# evaluate the perf
postResample(rf_predictions, validation_set$Sales)

# Do RMSPE
# Remove missing values from actual and predicted values
rf_actual_values <- validation_set$Sales[!is.na(validation_set$Sales)]
rf_predicted_values <- rf_predictions[!is.na(rf_predictions)]

# Replace zero values in predicted_values with a small constant
rf_predicted_values[rf_predicted_values == 0] <- 1e-10

# Calculate RMSPE using MLmetrics package
rf_RMSPE_value <- RMSPE(rf_actual_values, rf_predicted_values)

# Print RMSPE
cat("RMSPE:", round(rf_RMSPE_value*100, 2),"%", "\n")
```

Once this had run, evaluation statistics were calculated and can be seen in table @tbl-rf_perf_values. This shows a marked improvement in the R^2^ value, with 0.79, or 79% or the variation in predicted sales being able to be explained by the model. Effectiveness of the model was again evaluated with an `r paste0('RMSPE value of ', round(rf_RMSPE_value*100, 2), '%')` being calculated. This proved to be a slight improvement overall from the Decision Tree model as it fell just below the 0.5, or 50% threshold.

![Performance statistics from the Random Forest model](images/rf_perf_tbl.png){#tbl-rf_perf_values}

\FloatBarrier

```{r}
# Assuming your validation_set contains the Month column along with Sales
# Create a data frame with Month, Actual Sales, and Predicted Sales
plot_data_rf <- data.frame(Month = validation_set$Month,
                           Actual_Sales = validation_set$Sales,
                           Predicted_Sales = rf_predictions)

# Calculate average sales per month
avg_sales_rf <- plot_data_rf |> 
  group_by(Month)  |> 
  summarise(Avg_Actual_Sales = mean(Actual_Sales, na.rm = TRUE),
            Avg_Predicted_Sales = mean(Predicted_Sales, na.rm = TRUE))

library(ggplot2)
# Plot using ggplot2
ggplot(avg_sales_rf, aes(x = Month)) +
  geom_line(aes(y = Avg_Actual_Sales, color = "Actual Sales")) +
  geom_line(aes(y = Avg_Predicted_Sales, color = "Predicted Sales")) +
  labs(y = "Average Sales",
       color = "Sales Type") +
  scale_color_manual(values = c("Actual Sales" = "dodgerblue4", "Predicted Sales" = "firebrick")) +
  theme_minimal()
```

\FloatBarrier

If we compare the @fig-dt_sales above and @fig-rf_sales below, we can visualise this improvement, with the overall gap between actual and predicted sales shrinking slightly, but with these figures significantly closer together between months 1 and 2.

![Average actual vs predicted sales per month using a Random Forest model](images/act_pred_sales_month_rf.png){#fig-rf_sales width="300"}

\FloatBarrier

### XGBoost

The final non-linear model to be chosen for testing on the ability to predict sales was the eXtreme Gradient Boosting (XGBoost) model. This is another non-linear tree based machine learning algorithm, similar to the Decision Tree and Random Forest previously discussed. The major difference(s) is that, for example, the Random Forest will calculate the results using a number of 'trees' that have been produced in parallel, whilst the XGBoost model uses a tree and sequentially tries to improve on that one.

```{r}
# XGBoost
library(xgboost)
str(train_set)
str(validation_set)
# -Store, -WeekOfYear
 
train_matrix <- xgb.DMatrix(data = as.matrix(train_set |> select(-Sales)), label = train_set$Sales)
    
validation_matrix <- xgb.DMatrix(data = as.matrix(validation_set |> select(-Sales, )), label = validation_set$Sales)
 
# Set parameters
params <- list(
  objective = "reg:squarederror",  # For regression tasks
  eval_metric = "rmse",            # Root Mean Squared Error
  max_depth = 6,                   # Maximum depth of a tree
  eta = 0.1,                       # Learning rate
  subsample = 0.8,                 # Subsample ratio of the training instance
  colsample_bytree = 0.8           # Subsample ratio of columns when constructing each tree
)

# Train the model
set.seed(123)  # For reproducibility
xgb_model <- xgb.train(
  params = params,
  data = train_matrix,
  nrounds = 1000,               # Number of boosting rounds
  watchlist = list(train = train_matrix, validate = validation_matrix),
  early_stopping_rounds = 10,  # Stop if no improvement after 10 rounds
  verbose = 1                  # Print training log
)

# seems to best so far with 69.32%
# Predict on validation set
xgb_predictions <- predict(xgb_model, validation_matrix)

# evaluate the perf
postResample(xgb_predictions, validation_set$Sales)
saveRDS(xgb_predictions, file = "Data/xgb_predictions.rds")
```

Again, the model was trained and validated using the same datasets as before. The Root Mean Squared Error was used for the results calculation parameter, and the number of iterations was set to 1000.

```{r, eval=TRUE}
library(xgboost)
xgb_predictions <- readRDS("Data/xgb_predictions.rds")
# Do RMSPE
# Remove missing values from actual and predicted values
xg_actual_values <- validation_set$Sales[!is.na(validation_set$Sales)]
xg_predicted_values <- xgb_predictions[!is.na(xgb_predictions)]

# Replace zero values in predicted_values with a small constant
xg_predicted_values[xg_predicted_values == 0] <- 1e-10

# Calculate RMSPE using MLmetrics package
xg_RMSPE_value <- RMSPE(xg_actual_values, xg_predicted_values)

# Print RMSPE
cat("RMSPE:", round(xg_RMSPE_value*100, 2),"%", "\n")
```

Once the model had been trained and validated the R^2^ value was calculated to 0.92 or, again, 92% of the variation in predicted sales can be explained by the model. In addition, again, the RMSPE was also calculated to `r paste0(round(xg_RMSPE_value*100, 2), '%')` This is another improvement on the previous (Random Forest) model.

\FloatBarrier

![Performance statistics for the XGBoost model](images/xg_perf_tbl.png){#tbl-xg_perf_values fig-align="center"} \FloatBarrier

```{r}
# Assuming your validation_set contains the Month column along with Sales
# Create a data frame with Month, Actual Sales, and Predicted Sales
plot_data_xgb <- data.frame(Month = validation_set$Month,
                            Actual_Sales = validation_set$Sales,
                            Predicted_Sales = xgb_predictions)

# Calculate average sales per month
avg_sales_xgb <- plot_data_xgb |> 
  group_by(Month) |> 
  summarise(Avg_Actual_Sales = mean(Actual_Sales, na.rm = TRUE),
            Avg_Predicted_Sales = mean(Predicted_Sales, na.rm = TRUE))

library(ggplot2)
# Plot using ggplot2
ggplot(avg_sales_xgb, aes(x = Month)) +
  geom_line(aes(y = Avg_Actual_Sales, color = "Actual Sales")) +
  geom_line(aes(y = Avg_Predicted_Sales, color = "Predicted Sales")) +
  labs(y = "Average Sales",
       color = "Sales Type") +
  scale_color_manual(values = c("Actual Sales" = "dodgerblue4", "Predicted Sales" = "firebrick")) +
  theme_minimal()
```

If we look at @fig-sales_xgb we can again see a visualisation of the improvements of the XGBoost model. The 1^st^ 3 months were relatively similar, however there is a visual narrowing of the gap between the actual and predicted sales for the rest of the period being analysed.

\FloatBarrier

![Actual v predicted sales from the xgboost trained model](images/act_pred_sales_month_xgb.png){#fig-sales_xgb width="300"}

\FloatBarrier

### USE XGBOOST MODEL TO PREDICT SALES

With the above in mind, we decided that the model that proved to be able to provide the best predictions of 'Sales' was the XGBoost model. Therefore we would use that in the final prediction of sales for a period where sales were unknown as opposed to the training and validation of predictions using known sales.

```{r}
# Final Predictions using XGB_Model
# Join the store and tesdata
test_data <- read.csv("Data/clean_test_data.csv")
store_data <- read.csv("Data/clean_store_data.csv")
```

```{r}
# Merge the data
str(test_data) #41088
str(store_data) #1115

sales_prediction_data <- merge(test_data, store_data, by = "Store") |> 
  arrange(Year, Month)

str(sales_prediction_data) #41088
```

```{r}
# Use XGBoost model to predict the sales_prediction_data from the mdata
sales_prediction_data <- sales_prediction_data  |> select(Store, DayOfWeek, Open, Promo, StateHoliday_Bool, Year, Month, StoreType, Assortment, CompetitionDistance, Promo2, WeekOfYear, Sales)
str(sales_prediction_data)

sales_prediction_data$StoreType <- dplyr::recode(sales_prediction_data$StoreType, "a" = 1, "b" = 2, "c" = 3, "d" = 4)
str(sales_prediction_data)

sales_prediction_data$Assortment <- dplyr::recode(sales_prediction_data$Assortment, "a" = 1, "b" = 2, "c" = 3)
str(sales_prediction_data)


str(mdata)
mdata$StoreType <- dplyr::recode(mdata$StoreType, "a" = 1, "b" = 2, "c" = 3, "d" = 4)
str(mdata)

mdata$Assortment <- dplyr::recode(mdata$Assortment, "a" = 1, "b" = 2, "c" = 3)
str(mdata)

# Get the model matrix data
model_matrix <- xgb.DMatrix(data = as.matrix(mdata |> select(-Sales)), label = mdata$Sales)
str(model_matrix)

sales_prediction_data$Sales[is.na(sales_prediction_data$Sales)] <- 0
str(sales_prediction_data)

sales_prediction_matrix <- xgb.DMatrix(data = as.matrix(sales_prediction_data |> select(-Sales)), label = sales_prediction_data$Sales)
str(sales_prediction_matrix)

sales_prediction_data$Sales <- predict(xgb_model, sales_prediction_matrix)

str(sales_prediction_data)
```

# Plotting the final sales results

```{r}
# Plot the historic sales data for the stores that appear in both the actual and predicted sales data
# Get the stores that appear in both the actual sales data and predicted sales data
hist_sales <- df |>
  filter(df$Store %in% sales_prediction_data$Store)

hist_sales <- hist_sales |>
  filter(hist_sales$Month %in% sales_prediction_data$Month)

hist_sales <- hist_sales |>
  filter(hist_sales$WeekOfYear %in% sales_prediction_data$WeekOfYear)

# Get the predicted average sales per week
hist_sales <- hist_sales |>
  group_by(DayOfWeek) |>
  summarise(avg_hist_sales = mean(Sales, na.rm = TRUE))

# do the same for the sales_prediction_data
pred_avg_sales_per_week <- sales_prediction_data |>
  group_by(DayOfWeek) |>
  summarise(avg_pred_sales = mean(Sales, na.rm = TRUE))

library(ggplot2)
ggplot(hist_sales, aes(x = DayOfWeek, y = avg_hist_sales)) +
  geom_line(aes(y = avg_hist_sales,
                colour = "Actual Daily Sales")) +
  geom_line(data = pred_avg_sales_per_week,
            aes(y = avg_pred_sales, colour = "Predicted Daily Sales")) +
  scale_color_manual(values = c("Actual Daily Sales" = "dodgerblue4", "Predicted Daily Sales" = "firebrick")) +
  labs(y = "Average Sales",
       color = "Sales Type") +
  theme_minimal()
```

![Average actual vs predicted sales per week of year using the XGBoost model](images/final_act_pred_sales_month_xgb.png){#fig-final_xgb width="300"}

Review the available data and describe it in terms of its variables, quality, and relevance to the sales forecasting

Link data sets together as appropriate

Pre-process the data as appropriate for further analytics, for example, you may want to encode any categorical data, create new variables, identify how many missing values there are and deal with them appropriately, etc.

Identify the key factors affecting sales, for example, you may want to check whether competition and promotions have an impact on sales, and how public holidays cause sales fluctuations.

Build a forecasting model (which can be a linear regression model, a neural network model or something else) using the variables you identified. Please make sure to justify the choice of your modelling approach.

Use the Root Mean Square Percentage Error (RMSPE) to forecast accuracy

# Results

Interpret key results, assumptions and limitations of your analysis.

# Conclusion

## Limitations

## Implications

## Recommendations

\newpage

# References
