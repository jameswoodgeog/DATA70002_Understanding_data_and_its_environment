---
title: "Sales forecasting for the European drug store Rossman"
# Set the formatting options
format:
  pdf:
    pdf-engine: xelatex
# Include a Table Of Contents
    toc: true
# Include a List Of Figures
    lof: false
# Include a List Of Tables
    lot: false
# Number each headed section
    number-sections: true
# Set the main font size and font
    fontsize: 11pt
    mainfont: Calibri
# Adjust the borders to decrease or increase the useable page space
    geometry:
      - top = 15mm
      - bottom = 20mm
      - left = 20mm
      - right = 20mm
# Adjust the headers and footers
    header-includes:
# Set the packages to be used by LaTex
      - \usepackage{placeins}
      - \usepackage{fancyhdr}
      - \usepackage{lastpage}
#  Set the style and what goes in the header and footer of the main and all other pages.
      - \pagestyle{fancy}
      - \thispagestyle{fancy}
      - \fancyhead[R]{Student ID| 11600159}
      - \fancyhead[L]{DATA70002 | Understanding Data and its Environment}
      - \renewcommand{\headrulewidth}{0.02pt}
      - \fancypagestyle{plain}{\fancyhead[R]{Student ID| 11600159}\fancyhead[L]{DATA70002 | Understanding Data and its Environment}\fancyfoot[C]{Page \thepage\ of \pageref{LastPage}}}
# Set the page number to be "Page n of np"
      - \fancyfoot[C]{Page \thepage\ of \pageref{LastPage}}

# CSL and bibliography file to be used
bibliography: DATA70002_refs.bib
csl: "UoL_Harvard.csl"
---

```{=html}
<style>
figcaption {
    text-align: center;
}
</style>
```


```{r , echo=FALSE, include=FALSE, eval=FALSE, message=FALSE}
# Load the required libraries
library(tidyverse)
library(ggplot2)
library(dplyr)
library(corrplot)
library(car)
library(tree)
library(rpart)
library(caret)
library(MLmetrics)
library(randomForest)
library(xgboost)
library(kableExtra)
```

```{r , echo=FALSE, include=FALSE, eval=FALSE, message=FALSE}
# Load the data
df <- read.csv("Data/updated_training_data_with_store.csv")
```

\newpage

# Introduction

Predicting sales is a vital part for any business across all sectors, from manufacturing, retail, logistics, to wholesale. However, this is one of the most difficult tasks a business can undertake due to the complexities involved. Sales are driven by a great deal of different factors such as the store location, macro scale of yearly seasonality, to the micro scales of the time of day and the day of the week, whether there is a promotion or what the weather is doing [@hasanAddressingSeasonalityTrend2024]. All of these things influence sales in different ways, so as you can see, this makes forecasting sales the ultimate challenge for a business.

## Historic trend in sales

As a company Rossmann, a part of the A S Watson group, is the market leader for health and beauty retail in Germany with around 100 stores. It also has over 4,500 stores across Europe, from Poland, Turkey to Spain, employing over 60,000 people [@aswatsongroupRossmann2024]. We have been asked to

# Methodology

### Store data cleaning

Have a look at the structures of the stores data

Describe the steps used to

```{r , echo=FALSE, include=FALSE, eval=FALSE, message=FALSE}
# Recode the storetype and assortment columns
table(df$StoreType)
df$StoreType <- dplyr::recode(df$StoreType, "a" = 1, "b" = 2, "c" = 3, "d" = 4)
table(df$StoreType)

table(df$Assortment)
df$Assortment <- dplyr::recode(df$Assortment, "a" = 1, "b" = 2, "c" = 3)
table(df$Assortment)
```

```{r , echo=FALSE, include=FALSE, eval=FALSE, message=FALSE}
# correlation plot to find multicollinearity
numeric_df <- df[, sapply(df, is.numeric)]
cor_matrix <- cor(numeric_df)
corrplot(cor_matrix,
         type = "lower",
         method = "square",
         tl.srt = 45,
         order = "hclust",
         tl.cex = .5,
         tl.col = "midnightblue",
         col = COL2('BrBG', 10),
         addCoef.col = "grey50",
         number.cex = .45,
         cl.pos = "b",
         cl.length = 11,
         cl.ratio = 0.1)

```

```{r}
remove(numeric_df, cor_matrix)
```


```{r , echo=FALSE, include=FALSE, eval=FALSE, message=FALSE}
names(df)
```

```{r , echo=FALSE, include=FALSE, eval=FALSE, message=FALSE}
# create a new dataframe from df that does not include customers
mdata <- df  |> select(Store, Sales, DayOfWeek, Open, Promo, StateHoliday_Bool, Year, Month, StoreType, Assortment, CompetitionDistance, Promo2, WeekOfYear) |> 
  arrange(Year, Month)
```

```{r}
# create a linear model
formula <- Sales ~ DayOfWeek + Open + Promo + StateHoliday_Bool + Year + Month + StoreType + Assortment + CompetitionDistance + Promo2 + WeekOfYear

# Run the model and show the summary
m <- lm(formula, data = mdata)
summary(m)
```
```{r}
# tells me about multicollinearity - values less than 5 are OK
vif(m)
```
```{r}
# rerun the model without the variables that have a high VIF
# create a linear model
formula <- Sales ~ DayOfWeek + Open + Promo + StateHoliday_Bool + Year + Month + StoreType + Assortment + CompetitionDistance + Promo2

# Run the model and show the summary
m <- lm(formula, data = mdata)
summary(m)
```

```{r}
# tells me about multicollinearity - values less than 5 are OK
vif(m)
```

```{r , echo=FALSE, include=FALSE, eval=FALSE, message=FALSE}
hist(m$residuals)
```

```{r , echo=FALSE, include=FALSE, eval=FALSE, message=FALSE}
x=m$fitted.values
y=m$residuals
plot(lm(y~x), which = 2)
```

```{r}
remove(x, y)
```


```{r , echo=FALSE, include=FALSE, eval=FALSE, message=FALSE}
# Split the data into training and test sets using test = up to and including 2014, and validate = 2015
# Sort the data by year and month
mdata <- mdata |> arrange(Year, Month)
train_set <- subset(mdata, Year <= 2014)
validation_set <- subset(mdata, Year >= 2015)
```

### Decision Tree

```{r , echo=FALSE, include=FALSE, eval=FALSE, message=FALSE}
# Decision Tree
#library(rpart)
#library(caret)
#library(MLmetrics)

# use grid search for hyperparameter optimisation
tune_grid <- expand.grid(cp = seq(0.01, 0.1, by = 0.01))
dt_model <- train(formula,
                  data = train_set,
                  method = "rpart",
                  trControl = trainControl(method = "cv", number = 5),
                  tuneGrid = tune_grid)

# print(dt_model)

# Plot the variable importances
varImp(dt_model) |>  plot()
```

```{r}
tree_predictions <- predict(dt_model, newdata = validation_set)

# evaluate the perf
postResample(tree_predictions, validation_set$Sales)
```
```{r}
# Do RMSPE
# Remove missing values from actual and predicted values
dt_actual_values <- validation_set$Sales[!is.na(validation_set$Sales)]
dt_predicted_values <- tree_predictions[!is.na(validation_set$Sales)]

# Replace zero values in predicted_values with a small constant
dt_predicted_values[dt_predicted_values == 0] <- 1e-10

# Calculate RMSPE using MLmetrics package
dt_RMSPE_value <- RMSPE(dt_actual_values, dt_predicted_values)

# Print RMSPE
cat("RMSPE:", round(dt_RMSPE_value*100, 2),"%", "\n")
```

```{r , echo=FALSE, include=FALSE, eval=FALSE, message=FALSE}
# # plot the actual v's predicted sales for this model
# tree_plot_data <- data.frame(actual = validation_set$Sales, predicted = tree_predictions) |> 
#   mutate(percentage_error = (actual - predicted) / actual * 100,
#          squared_percentage_error = percentage_error^2)
```

```{r , echo=FALSE, include=FALSE, eval=FALSE, message=FALSE}
# compare the actual vs predicted values
# ggplot(tree_plot_data, aes(x = actual, y = predicted)) +
#   geom_point() +
#   geom_abline(intercept = 0, slope = 1, color = "red") +
#   labs(title = "Actual vs Predicted Sales",
#        x = "Actual Sales",
#        y = "Predicted Sales")

```

### Random Forest

```{r , echo=FALSE, include=FALSE, eval=FALSE, message=FALSE}
# Random Forest model
# library(randomForest)
rf_model <- randomForest(formula,
                         data = train_set,
                         ntree = 10,
                         mtry = 3)

print(rf_model)
# seems to best so far with 69.32%
```

```{r}
# Predictions
rf_predictions <- predict(rf_model, newdata = validation_set)

# evaluate the perf
postResample(rf_predictions, validation_set$Sales)

# Do RMSPE
# Remove missing values from actual and predicted values
rf_actual_values <- validation_set$Sales[!is.na(validation_set$Sales)]
rf_predicted_values <- rf_predictions[!is.na(rf_predictions)]

# Replace zero values in predicted_values with a small constant
rf_predicted_values[rf_predicted_values == 0] <- 1e-10

# Calculate RMSPE using MLmetrics package
rf_RMSPE_value <- RMSPE(rf_actual_values, rf_predicted_values)

# Print RMSPE
cat("RMSPE:", round(rf_RMSPE_value*100, 2),"%", "\n")
```
### XGBoost

```{r , echo=FALSE, include=FALSE, eval=FALSE, message=FALSE}
# XGBoost
# library(xgboost)

str(train_set)
str(validation_set)
 
train_matrix <- xgb.DMatrix(data = as.matrix(train_set |> select(-Sales)), label = train_set$Sales)
str(train_set)

validation_matrix <- xgb.DMatrix(data = as.matrix(validation_set |> select(-Sales)), label = validation_set$Sales)
 
# Set parameters
params <- list(
  objective = "reg:squarederror",  # For regression tasks
  eval_metric = "rmse",            # Root Mean Squared Error
  max_depth = 6,                   # Maximum depth of a tree
  eta = 0.1,                       # Learning rate
  subsample = 0.8,                 # Subsample ratio of the training instance
  colsample_bytree = 0.8           # Subsample ratio of columns when constructing each tree
)
 
# simple model which somehow lets the below model run
set.seed(123)
xgb_model <- xgb.train(
  params = params,
  data = train_matrix,
  nrounds = 1000,
  verbose = 1
)
 
# Train the model
set.seed(123)  # For reproducibility
xgb_model <- xgb.train(
  params = params,
  data = train_matrix,
  nrounds = 1000,               # Number of boosting rounds
  watchlist = list(train = train_matrix, validate = validation_matrix),
  early_stopping_rounds = 10,  # Stop if no improvement after 10 rounds
  verbose = 1                  # Print training log
)

# seems to best so far with 69.32%
# Predict on validation set
xgb_predictions <- predict(xgb_model, validation_matrix)

# evaluate the perf
postResample(xgb_predictions, validation_set$Sales)
```

```{r}
# Do RMSPE
# Remove missing values from actual and predicted values
xg_actual_values <- validation_set$Sales[!is.na(validation_set$Sales)]
xg_predicted_values <- xgb_predictions[!is.na(xgb_predictions)]

# Replace zero values in predicted_values with a small constant
xg_predicted_values[xg_predicted_values == 0] <- 1e-10

# Calculate RMSPE using MLmetrics package
xg_RMSPE_value <- RMSPE(xg_actual_values, xg_predicted_values)

# Print RMSPE
cat("RMSPE:", round(xg_RMSPE_value*100, 2),"%", "\n")
```

### USE XGBOOST MODEL TO PREDICT SALES

```{r , echo=FALSE, include=FALSE, eval=FALSE, message=FALSE}
# Final Predictions using XGB_Model
# Join the store and tesdata
test_data <- read.csv("Data/clean_test_data.csv")
store_data <- read.csv("Data/clean_store_data.csv")
```

```{r , echo=FALSE, include=FALSE, eval=FALSE, message=FALSE}
# Merge the data
sales_prediction_data <- merge(test_data, store_data, by = "Store") |> 
  arrange(Year, Month)
```

```{r , echo=FALSE, include=FALSE, eval=FALSE, message=FALSE}
# Use XGBoost model to predict the sales_prediction_data from the mdata
sales_prediction_data <- sales_prediction_data  |> select(Sales, DayOfWeek, Open, Promo, StateHoliday_Bool, Year, Month, StoreType, Assortment, CompetitionDistance, Promo2)

sales_prediction_data$StoreType <- dplyr::recode(sales_prediction_data$StoreType, "a" = 1, "b" = 2, "c" = 3, "d" = 4)

sales_prediction_data$Assortment <- dplyr::recode(sales_prediction_data$Assortment, "a" = 1, "b" = 2, "c" = 3)

model_matrix <- xgb.DMatrix(data = as.matrix(mdata |> select(-Sales)), label = mdata$Sales)
str(train_set)

sales_prediction_data$Sales[is.na(sales_prediction_data$Sales)] <- 0

sales_prediction_matrix <- xgb.DMatrix(data = as.matrix(sales_prediction_data |> select(-Sales)), label = sales_prediction_data$Sales)

str(sales_prediction_data)
sales_prediction_data$Sales <- predict(xgb_model, sales_prediction_matrix)
```

```{r , echo=FALSE, include=FALSE, eval=FALSE, message=FALSE}
# Plot a yearly timeseries of sales for the muppets in Rossmann, and use both datasets (sales_prediction and mdata)
plot_data <- mdata |> 
  mutate(Month_Year = format(as.Date(paste0(Year, "-", Month, "-01")), "%b %Y"))  |> 
  group_by(Month_Year)  |> 
  summarise(Total_Sales_Month = sum(Sales),
            Year = first(Year),
            Month = first(Month)) |> 
  arrange(Year, Month)

plot_data <- plot_data |> 
  mutate(Month_Year = as.Date(paste0(Year, "-", Month, "-01"), "%Y-%m-%d"))

pred_plot_data <- sales_prediction_data |> 
  mutate(Month_Year = format(as.Date(paste0(Year, "-", Month, "-01")), "%b %Y"))  |> 
  group_by(Month_Year)  |> 
  summarise(Total_Sales_Month = sum(Sales),
            Year = first(Year),
            Month = first(Month)) |> 
  arrange(Year, Month)

pred_plot_data <- pred_plot_data |> 
  mutate(Month_Year = as.Date(paste0(Year, "-", Month, "-01"), "%Y-%m-%d"))


ggplot(plot_data, aes(x = Month_Year, y = Total_Sales_Month)) +
  geom_line(data = pred_plot_data, aes(x = Month_Year, y = Total_Sales_Month, color = "Predicted Sales")) +
  geom_point() +
  geom_line() +
  geom_point() +
  labs(title = "Total Sales by Month and Year",
       x = "Month-Year",
       y = "Total Sales") +
  scale_x_date(date_breaks = "6 months",
               date_labels = "%b %Y",
               minor_breaks = "month",
               labels = NULL) +
  scale_y_continuous(labels = scales::comma)




```

Review the available data and describe it in terms of its variables, quality, and relevance to the sales forecasting

Link data sets together as appropriate

Pre-process the data as appropriate for further analytics, for example, you may want to encode any categorical data, create new variables, identify how many missing values there are and deal with them appropriately, etc.

Identify the key factors affecting sales, for example, you may want to check whether competition and promotions have an impact on sales, and how public holidays cause sales fluctuations.

Build a forecasting model (which can be a linear regression model, a neural network model or something else) using the variables you identified. Please make sure to justify the choice of your modelling approach.

Use the Root Mean Square Percentage Error (RMSPE) to forecast accuracy

# Results

Interpret key results, assumptions and limitations of your analysis.

# Conclusion

## Limitations

## Implications

## Recommendations

# References
