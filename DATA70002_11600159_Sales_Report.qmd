---
title: "Sales forecasting for the European drug store Rossman"
# Set the formatting options
format:
  pdf:
    pdf-engine: xelatex
# Include a Table Of Contents
    toc: true
# Include a List Of Figures
    lof: false
# Include a List Of Tables
    lot: false
# Number each headed section
    number-sections: true
# Set the main font size and font
    fontsize: 11pt
    mainfont: Calibri
# Adjust the borders to decrease or increase the useable page space
    geometry:
      - top = 15mm
      - bottom = 20mm
      - left = 20mm
      - right = 20mm
# Adjust the headers and footers
    header-includes:
# Set the packages to be used by LaTex
      - \usepackage{placeins}
      - \usepackage{fancyhdr}
      - \usepackage{lastpage}
#  Set the style and what goes in the header and footer of the main and all other pages.
      - \pagestyle{fancy}
      - \thispagestyle{fancy}
      - \fancyhead[R]{Student ID| 11600159}
      - \fancyhead[L]{DATA70002 | Understanding Data and its Environment}
      - \renewcommand{\headrulewidth}{0.02pt}
      - \fancypagestyle{plain}{\fancyhead[R]{Student ID| 11600159}\fancyhead[L]{DATA70002 | Understanding Data and its Environment}\fancyfoot[C]{Page \thepage\ of \pageref{LastPage}}}
# Set the page number to be "Page n of np"
      - \fancyfoot[C]{Page \thepage\ of \pageref{LastPage}}

# CSL and bibliography file to be used
bibliography: DATA70002_refs.bib
csl: "UoL_Harvard.csl"
---

```{=html}
<style>
figcaption {
    text-align: center;
}
</style>
```

```{r , echo=FALSE, include=FALSE, eval=FALSE, message=FALSE}
# Load the required libraries
library(tidyverse)
library(ggplot2)
library(dplyr)
library(corrplot)
```

```{r , echo=FALSE, include=FALSE, eval=FALSE, message=FALSE}
# Load the data
df <- read.csv("Data/updated_training_data_with_store.csv")
```


\newpage

# Introduction

# Methodology

### Store data cleaning
```{r, eval=FALSE, include=FALSE}
table(df$StoreType)
df$StoreType <- recode(df$StoreType, "a" = 1, "b" = 2, "c" = 3, "d" = 4)
table(df$StoreType)

table(df$Assortment)
df$Assortment <- recode(df$Assortment, "a" = 1, "b" = 2, "c" = 3)
table(df$Assortment)
```

```{r, eval=FALSE, include=FALSE}
# correlation plot to find multicollinearity
numeric_df <- df[, sapply(df, is.numeric)]
cor_matrix <- cor(numeric_df)
corrplot(cor_matrix,
         type = "lower",
         method = "square",
         tl.srt = 45,
         order = "hclust",
         tl.cex = .5,
         tl.col = "midnightblue",
         col = COL2('BrBG', 10),
         addCoef.col = "grey50",
         number.cex = .45,
         cl.pos = "b",
         cl.length = 11,
         cl.ratio = 0.1)
```
```{r}
formula <- Sales ~ DayOfWeek + Open + Promo + StateHoliday + Year + Month + StoreType + Assortment + CompetitionDistance + Promo2
m <- lm(formula, data = df)
summary(m)

library(car)
vif(m)
# tells me about multicollinearity - values less than 5 are OK

```

```{r}
hist(m$residuals)
```
```{r}
x=m$fitted.values
y=m$residuals
plot(lm(y~x), which = 2)
```
```{r}
# remove customers from df
df <- df  |> select(-Customers)
# Decision Tree
library(tree)
# Split the data into training and test sets
set.seed(1234)
train_index <- sample(1:nrow(df), nrow(df)*0.7)

# train dataset formation
train_set <- df[train_index, ]
str(train_set)

validation_set <- df[-train_index, ]

library(rpart)
library(caret)
# use grid search for hyperparameter optimisation
tune_grid <- expand.grid(cp = seq(0.01, 0.1, by = 0.01))
dt_model <- train(formula,
                  data = train_set,
                  method = "rpart",
                  trControl = trainControl(method = "cv", number = 5),
                  tuneGrid = tune_grid)

print(dt_model)

predictions <- predict(dt_model, newdata = validation_set)

# evaluate the perf
postResample(predictions, validation_set$Sales)
```

```{r}
# Random Forest model
library(randomForest)
rf_model <- randomForest(formula,
                         data = train_set,
                         ntree = 10,
                         mtry = 3)

print(rf_model)
# seems to best so far with 69.32%
predictions <- predict(rf_model, newdata = validation_set)

# evaluate the perf
postResample(predictions, validation_set$Sales)
```
```{r}
# XGBoost
install.packages("xgboost", dependencies = T)
library(xgboost)
train_set$StateHoliday <- as.factor(train_set$StateHoliday)
validation_set$StateHoliday <- as.factor(validation_set$StateHoliday)

train_set$Date <- as.factor(train_set$Date)
validation_set$Date <- as.factor(validation_set$Date)

train_set$PromoInterval <- as.factor(train_set$PromoInterval)
validation_set$PromoInterval <- as.factor(validation_set$PromoInterval)

train_set$start_date <- as.factor(train_set$start_date)
validation_set$start_date <- as.factor(validation_set$start_date)


# Identify categorical and continuous columns
categorical_cols <- c('DayOfWeek', 'Open', 'Promo', 'StateHoliday', 'StoreType', 'Assortment', 'Year', 'Month')
continuous_cols <- c('CompetitionDistance', 'Promo2')

# One-hot encode categorical variables
trainData_cat <- model.matrix(~ DayOfWeek + Open + Promo + StateHoliday + Year + Month + StoreType + Assortment - 1, data = trainData)
testData_cat <- model.matrix(~ DayOfWeek + Open + Promo + StateHoliday + Year + Month + StoreType + Assortment - 1, data = testData)


trainData <- model.matrix(~ DayOfWeek + Open + Promo + StateHoliday + Year + Month + StoreType + Assortment + CompetitionDistance + Promo2, data = train_set)
testData <- model.matrix(~ DayOfWeek + Open + Promo + StateHoliday + Year + Month + StoreType + Assortment + CompetitionDistance + Promo2, data = validation_set)

xgb_model <- xgboost(data = as.matrix(train_set[, -1]),
                     label = train_set$Sales,
                     nrounds = 10,
                     objective = "reg:linear",
                     verbose = 0)

str(train_set)

```












Review the available data and describe it in terms of its variables, quality, and relevance to the sales forecasting
































Link data sets together as appropriate

Pre-process the data as appropriate for further analytics, for example, you may want to encode any categorical data, create new variables, identify how many missing values there are and deal with them appropriately, etc.

Identify the key factors affecting sales, for example, you may want to check whether competition and promotions have an impact on sales, and how public holidays cause sales fluctuations.

Build a forecasting model (which can be a linear regression model, a neural network model or something else) using the variables you identified. Please make sure to justify the choice of your modelling approach.

Use the Root Mean Square Percentage Error (RMSPE) to forecast accuracy

# Results

Interpret key results, assumptions and limitations of your analysis.

# Conclusion

## Limitations

## Implications

## Recommendations

# References
